[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Overview\nArchitecture\nCore Concepts\nConfiguration Schema\nExpression Language\nGlobal Tasks\nParameter Scoping\nDevelopment Mode\nProduction Deployment\nUI Design\nData Model\nImplementation Details\nTesting Strategy\nDeployment Guide\n\n\n\n\n\n\n\nA lightweight, portable, event-driven workflow orchestrator that gets out of your way. Designed for data teams who want to ship fast without operational overhead.\nCore Philosophy: - Simple over complex - Portable over locked-in - YAML over code - Files over databases (for events) - Commands over plugins\n\n\n\n\nGlobal Task Deduplication - Run once, notify many graphs\nSingle Binary - No dependencies, runs anywhere\nFile-Based Events - JSONL files, not databases\nMulti-Threaded Workers - Efficient for I/O-bound tasks\nDev Mode - Single process with embedded worker\nTrial Run - See what would execute without executing\n\n\n\n\n\nData engineering teams (5-50 people)\nOrganizations with multi-cloud or hybrid deployments\nTeams that value simplicity and portability\nCost-conscious organizations\nEdge computing scenarios\n\n\n\n\n\nFAANG-scale (millions of tasks/day)\nTeams that need complex multi-tenancy\nMicroservice orchestration (use Temporal)\nTeams deeply invested in Airflow ecosystem\n\n\n\n\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│                    Orchestrator Process                     │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │              Core Orchestration                        │ │\n│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐   │ │\n│  │  │Graph Loader  │  │Event Bus     │  │Graph        │   │ │\n│  │  │(YAML→Memory) │  │(Vert.x)      │  │Evaluator    │   │ │\n│  │  └──────────────┘  └──────────────┘  └─────────────┘   │ │\n│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐   │ │\n│  │  │State Manager │  │Worker Monitor│  │Task Queue   │   │ │\n│  │  │(Hazelcast)   │  │              │  │Publisher    │   │ │\n│  │  └──────────────┘  └──────────────┘  └─────────────┘   │ │\n│  └────────────────────────────────────────────────────────┘ │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │              Worker Pool (Embedded in Dev)             │ │\n│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐    │ │\n│  │  │Thread 1 │  │Thread 2 │  │Thread 3 │  │Thread 4 │    │ │\n│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘    │ │\n│  └────────────────────────────────────────────────────────┘ │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │                    Web UI (SSR)                        │ │\n│  │  ┌──────────────────────────────────────────────────┐  │ │\n│  │  │  Qute Templates + Tabler CSS + Monaco Editor     │  │ │\n│  │  └──────────────────────────────────────────────────┘  │ │\n│  └────────────────────────────────────────────────────────┘ │\n└─────────────────────────────────────────────────────────────┘\n                              │\n                ┌─────────────┴─────────────┐\n                ↓                           ↓\n        ┌──────────────┐          ┌──────────────────┐\n        │ Event Store  │          │  Config Files    │\n        │ (JSONL)      │          │  (YAML)          │\n        │              │          │                  │\n        │ Local: files │          │  graphs/*.yaml   │\n        │ Prod: GCS/S3 │          │  tasks/*.yaml    │\n        └──────────────┘          └──────────────────┘\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│                         GKE Cluster                         │\n│                                                             │\n│  ┌──────────────────┐         ┌─────────────────────────┐   │\n│  │   Orchestrator   │         │   Worker Pods (HPA)     │   │\n│  │    (1 replica)   │         │                         │   │\n│  │                  │         │  ┌────────┐ ┌────────┐  │   │\n│  │  - Graph Eval    │         │  │Worker 1│ │Worker 2│  │   │\n│  │  - State Mgmt    │         │  │16 thrd │ │16 thrd │  │   │\n│  │  - UI (SSR)      │         │  └────────┘ └────────┘  │   │\n│  │  - API           │         │       ...               │   │\n│  └──────────────────┘         └─────────────────────────┘   │\n│         │                                                   │\n└─────────┼───────────────────────────────────────────────────┘\n          │\n    ┌─────┴─────┐\n    ▼           ▼\n┌────────┐  ┌────────────┐      ┌──────────────┐\n│Hazel-  │  │  GCS/S3    │      │ ConfigMap    │\n│cast    │  │            │      │              │\n│Cluster │  │ Events:    │      │ graphs/*.yaml│\n│(3 AZs) │  │ *.jsonl    │      │ tasks/*.yaml │\n└────────┘  └────────────┘      └──────────────┘\n\n\n\n\n\n\n\nDefinition: A single unit of work. Just a command to execute.\nKey Properties: - name - Unique identifier - command - Program to execute - args - Command arguments (with JEXL expressions) - env - Environment variables - timeout - Maximum execution time - retry - Retry policy\nExample:\nname: extract-data\ncommand: python\nargs:\n  - /scripts/extract.py\n  - --date\n  - \"${params.batch_date}\"\nenv:\n  PYTHONUNBUFFERED: \"1\"\n  API_KEY: \"${env.API_KEY}\"\ntimeout: 600  # 10 minutes\nretry: 3\n\n\n\nDefinition: Tasks that can be shared across multiple graphs. Run once per unique parameter set.\nKey Feature: Deduplication via parameterized keys.\nKey Properties: - global: true - Marks as global - key - Expression that uniquely identifies this task instance - params - Parameters with defaults\nExample:\nname: load-market-data\nglobal: true\nkey: \"load_market_${params.batch_date}_${params.region}\"\n\nparams:\n  batch_date:\n    type: string\n    required: true\n  region:\n    type: string\n    default: us\n\ncommand: dbt\nargs:\n  - run\n  - --models\n  - +market_data\n  - --vars\n  - \"batch_date:${params.batch_date},region:${params.region}\"\nBehavior: - Graph A needs load-market-data[2025-10-17, us] - Graph B needs load-market-data[2025-10-17, us] - Task runs once, both graphs notified on completion - Graph C needs load-market-data[2025-10-16, us] - Different key → runs separately\n\n\n\nDefinition: A directed acyclic graph (DAG) of tasks with dependencies.\nKey Properties: - name - Unique identifier - params - Parameters with defaults (can be overridden at runtime) - env - Graph-level environment variables - tasks - List of tasks (inline or references to global tasks) - schedule (optional) - Cron expression for scheduled execution\nExample:\nname: daily-etl\ndescription: Daily ETL pipeline\n\nparams:\n  batch_date:\n    type: string\n    default: \"${date.today()}\"\n  region:\n    type: string\n    default: us\n\nenv:\n  GCS_BUCKET: \"gs://data-${params.region}\"\n  PROCESSING_DATE: \"${params.batch_date}\"\n\nschedule: \"0 2 * * *\"  # 2 AM daily\n\ntasks:\n  # Reference global task\n  - task: load-market-data\n    params:\n      batch_date: \"${params.batch_date}\"\n      region: \"${params.region}\"\n  \n  # Inline task\n  - name: transform-data\n    command: python\n    args:\n      - /scripts/transform.py\n      - --input\n      - \"${env.GCS_BUCKET}/raw/${params.batch_date}\"\n    depends_on:\n      - load-market-data\n\n\n\nDefinition: A single run of a graph with specific parameters.\nLifecycle: 1. RUNNING - Graph is being evaluated/executed 2. COMPLETED - All tasks completed successfully 3. FAILED - One or more tasks failed permanently 4. STALLED - No progress possible (waiting on failed dependencies) 5. PAUSED - Manually paused by user\n\n\n\nDefinition: A single execution of a task within a graph execution.\nLifecycle: 1. PENDING - Task created, waiting for dependencies 2. QUEUED - Published to worker queue 3. RUNNING - Worker is executing 4. COMPLETED - Successfully finished 5. FAILED - Failed (may retry) 6. SKIPPED - Skipped due to upstream failure\n\n\n\nDefinition: Processes that pull tasks from queue and execute them.\nTypes: - Embedded Worker (dev mode) - Runs in same process as orchestrator - Dedicated Workers (prod) - Separate pods with multi-threaded execution\nKey Properties: - worker_id - Unique identifier (hostname/pod name) - threads - Number of concurrent task executions - heartbeat - Periodic health signal (every 10s) - capabilities - What tasks can run (future: heterogeneous workers)\n\n\n\n\n\n\n\nLocation: tasks/{task-name}.yaml\n# Required: Task name (must match filename without .yaml)\nname: string  # Pattern: ^[a-z0-9-]+$\n\n# Optional: Mark as global task\nglobal: boolean  # Default: false\n\n# Required for global tasks: Unique key expression\nkey: string  # JEXL expression with params\n\n# Optional: Parameters (required for global tasks with key)\nparams:\n  param_name:\n    type: string|integer|boolean|date|array  # Required\n    default: any  # Optional (JEXL expression allowed)\n    required: boolean  # Default: false\n    description: string  # Optional\n\n# Required: Command to execute\ncommand: string\n\n# Required: Command arguments\nargs:\n  - string  # JEXL expressions allowed\n\n# Optional: Environment variables\nenv:\n  KEY: string  # JEXL expressions allowed\n\n# Optional: Timeout in seconds\ntimeout: integer  # Default: 3600 (1 hour)\n\n# Optional: Retry policy\nretry: integer  # Default: 3 (max retry attempts)\n\n\n\nLocation: graphs/{graph-name}.yaml\n# Required: Graph name (must match filename without .yaml)\nname: string  # Pattern: ^[a-z0-9-]+$\n\n# Optional: Human-readable description\ndescription: string\n\n# Optional: Parameters\nparams:\n  param_name:\n    type: string|integer|boolean|date|array\n    default: any  # JEXL expression allowed\n    required: boolean  # Default: false\n    description: string\n\n# Optional: Graph-level environment variables\nenv:\n  KEY: string  # JEXL expressions allowed\n\n# Optional: Schedule (cron expression)\nschedule: string  # e.g., \"0 2 * * *\"\n\n# Optional: Triggers\ntriggers:\n  - type: webhook|pubsub|schedule\n    # Type-specific config\n\n# Required: Tasks\ntasks:\n  # Option 1: Reference global task\n  - task: string  # Global task name\n    params:  # Optional param overrides\n      param_name: any  # JEXL expression\n    depends_on:  # Optional\n      - string  # Task names\n  \n  # Option 2: Inline task definition\n  - name: string\n    command: string\n    args:\n      - string\n    env:\n      KEY: string\n    timeout: integer\n    retry: integer\n    depends_on:\n      - string\n\n\n\nLocation: application.yaml (or application-{profile}.yaml)\norchestrator:\n  # Mode: dev or prod\n  mode: dev\n  \n  # Config paths\n  config:\n    graphs: ./graphs  # Directory containing graph YAML files\n    tasks: ./tasks    # Directory containing task YAML files\n    watch: true       # Watch for file changes (dev only)\n  \n  # Development mode settings\n  dev:\n    worker-threads: 4       # Embedded worker thread count\n    trial-run: false        # If true, only log commands instead of executing\n    enable-editor: true     # Enable web-based YAML editor\n  \n  # Storage\n  storage:\n    events: file://./data/events  # or gs://bucket/events or s3://bucket/events\n  \n  # Hazelcast\n  hazelcast:\n    embedded: true  # true for dev, false for prod\n    cluster-name: orchestrator\n    members:        # Prod only\n      - orchestrator-0.orchestrator\n      - orchestrator-1.orchestrator\n      - orchestrator-2.orchestrator\n  \n  # Worker settings (for dedicated workers)\n  worker:\n    threads: 16             # Threads per worker pod\n    heartbeat-interval: 10  # Seconds\n    dead-threshold: 60      # Seconds\n\n# Web server\nserver:\n  port: 8080\n\n# Logging\nlogging:\n  level: info\n  format: json  # or text\n\n\n\n\n\n\n\nWe use Apache Commons JEXL 3 for all expressions. Expressions are wrapped in ${ }.\nKey Features: - Arithmetic: +, -, *, /, % - Comparison: ==, !=, &lt;, &gt;, &lt;=, &gt;= - Logical: &&, ||, ! - Ternary: condition ? true_val : false_val - Null-safe: object?.property - Elvis: value ?: default\n\n\n\n${params.name}          - Access parameter\n${env.NAME}             - Access environment variable\n${task.taskname.result} - Access upstream task result (JSON)\n\n\n\n\n\n${date.today()}                              → \"2025-10-17\"\n${date.now('yyyy-MM-dd HH:mm:ss')}          → \"2025-10-17 14:30:00\"\n${date.add(date.today(), 1, 'days')}        → \"2025-10-18\"\n${date.sub(date.today(), 7, 'days')}        → \"2025-10-10\"\n${date.format(params.date, 'yyyy/MM/dd')}   → \"2025/10/17\"\n\n\n\n${string.uuid()}                             → \"550e8400-e29b-41d4-a716-446655440000\"\n${string.slugify('My Workflow Name')}       → \"my-workflow-name\"\n\n\n\n${'hello'.toUpperCase()}                     → \"HELLO\"\n${'HELLO'.toLowerCase()}                     → \"hello\"\n${'2025-10-17'.replace('-', '_')}           → \"2025_10_17\"\n${'  text  '.trim()}                         → \"text\"\n\n\n\n${Math.round(3.7)}                           → 4\n${Math.floor(3.7)}                           → 3\n${Math.ceil(3.2)}                            → 4\n${Math.max(10, 20)}                          → 20\n${Math.min(10, 20)}                          → 10\n\n\n\n\n# Simple variable substitution\ncommand: echo\nargs:\n  - \"Processing ${params.region}\"\n\n# Arithmetic\nenv:\n  BATCH_SIZE: \"${params.scale * 100}\"\n  WORKER_COUNT: \"${params.scale * 4}\"\n\n# Conditionals\nargs:\n  - \"${params.full_refresh ? '--full-refresh' : '--incremental'}\"\n\n# Complex logic\nenv:\n  PARALLELISM: \"${params.scale &gt; 10 ? 32 : params.scale &gt; 5 ? 16 : 4}\"\n\n# String manipulation\nenv:\n  TABLE_NAME: \"${'data_' + params.region + '_' + params.date.replace('-', '_')}\"\n\n# Null-safe access\nenv:\n  API_KEY: \"${params.config?.api?.key ?: env.DEFAULT_API_KEY}\"\n\n# Date operations\nparams:\n  yesterday:\n    default: \"${date.add(date.today(), -1, 'days')}\"\n\n# Reference upstream task output\nargs:\n  - --row-count\n  - \"${task.extract.result.row_count}\"\n\n\n\n\n\n\n\nGlobal tasks solve the problem of multiple graphs needing the same data/computation for the same parameters.\nWithout global tasks:\nGraph A: load-data[2025-10-17] → runs\nGraph B: load-data[2025-10-17] → runs (duplicate!)\nGraph C: load-data[2025-10-17] → runs (duplicate!)\nWith global tasks:\nGraph A: load-data[2025-10-17] → starts execution\nGraph B: load-data[2025-10-17] → links to same execution\nGraph C: load-data[2025-10-17] → links to same execution\n→ Runs once, all three graphs proceed together\n\n\n\nThe key field uniquely identifies a task instance. It should include all parameters that affect the task’s output.\nGood Keys:\n# Includes date and region - different dates/regions = different data\nkey: \"load_data_${params.batch_date}_${params.region}\"\n\n# Includes all meaningful params\nkey: \"bootstrap_curves_${params.date}_${params.region}_${params.model_version}\"\nBad Keys:\n# Too generic - all graphs share same execution even with different params\nkey: \"load_data\"\n\n# Includes volatile data - every execution is \"unique\"\nkey: \"load_data_${params.batch_date}_${date.now()}\"\n\n\n\nGlobal tasks are tracked in Hazelcast:\n// Key structure\nrecord TaskExecutionKey(\n    String taskName,\n    String resolvedKey  // Evaluated expression\n) {}\n\n// State\nrecord GlobalTaskExecution(\n    UUID id,\n    String taskName,\n    String resolvedKey,\n    Map&lt;String, Object&gt; params,\n    TaskStatus status,\n    Set&lt;UUID&gt; linkedGraphExecutions,  // Which graphs are waiting\n    Instant startedAt,\n    Instant completedAt,\n    Map&lt;String, Object&gt; result\n) {}\n\n\n\n\nGraph A evaluates, needs global task with key load_data_2025-10-17_us\nCheck Hazelcast for existing execution with that key\nNot found → Start new execution, add Graph A to linked graphs\nGraph B evaluates, needs same key\nFound, status=RUNNING → Add Graph B to linked graphs, don’t start new execution\nTask completes → Notify both Graph A and Graph B\nBoth graphs re-evaluate and schedule downstream tasks\n\n\n\n\n\n\n\n\n\nRuntime Invocation - Params passed when triggering graph\nGraph Task Reference - Params in graph’s task definition\nGlobal Task Defaults - Defaults in global task definition\nGraph Defaults - Defaults in graph params\n\n\n\n\nGlobal Task:\n# tasks/process-data.yaml\nname: process-data\nglobal: true\nkey: \"process_${params.date}_${params.region}\"\n\nparams:\n  date:\n    type: string\n    required: true\n  region:\n    type: string\n    default: us      # Level 3: Global task default\n  threads:\n    type: integer\n    default: 4       # Level 3: Global task default\nGraph:\n# graphs/etl-pipeline.yaml\nname: etl-pipeline\n\nparams:\n  date:\n    type: string\n    default: \"${date.today()}\"  # Level 4: Graph default\n  region:\n    type: string\n    default: eu                 # Level 4: Graph default (overrides global)\n\ntasks:\n  - task: process-data\n    params:\n      date: \"${params.date}\"\n      region: \"${params.region}\"\n      threads: 8                # Level 2: Task reference override\nRuntime Invocation:\ncurl -X POST /api/graphs/etl-pipeline/execute \\\n  -d '{\"params\": {\"date\": \"2025-10-15\", \"region\": \"asia\"}}'\n# Level 1: Runtime override (highest priority)\nFinal Resolution:\ndate: \"2025-10-15\"    (Level 1: runtime)\nregion: \"asia\"        (Level 1: runtime)\nthreads: 8            (Level 2: task reference)\n\n\n\n\n\n\n\nIn dev mode, everything runs in one process:\n┌─────────────────────────────────────────┐\n│      Single JVM Process                 │\n│                                         │\n│  Orchestrator Core                      │\n│  ↓                                      │\n│  Embedded Hazelcast                     │\n│  ↓                                      │\n│  Embedded Worker Pool (4 threads)       │\n│  ↓                                      │\n│  Local Event Store (./data/events)      │\n│  ↓                                      │\n│  Web UI (http://localhost:8080)         │\n└─────────────────────────────────────────┘\nStart command:\n./orchestrator --profile=dev\n# or\njava -jar orchestrator.jar --profile=dev\nOutput:\n[INFO] Orchestrator starting in DEV mode\n[INFO] Loading graphs from ./graphs\n[INFO]   - daily-etl.yaml\n[INFO]   - market-pipeline.yaml\n[INFO] Loading tasks from ./tasks\n[INFO]   - load-market-data.yaml\n[INFO]   - bootstrap-curves.yaml\n[INFO] Starting embedded Hazelcast cluster\n[INFO] Hazelcast cluster formed (1 member)\n[INFO] Starting embedded worker pool with 4 threads\n[INFO]   - worker-thread-1 ready\n[INFO]   - worker-thread-2 ready\n[INFO]   - worker-thread-3 ready\n[INFO]   - worker-thread-4 ready\n[INFO] Starting web server on port 8080\n[INFO] \n[INFO] ============================================\n[INFO] Orchestrator ready!\n[INFO] UI: http://localhost:8080\n[INFO] Mode: DEVELOPMENT\n[INFO] Trial Run: DISABLED\n[INFO] ============================================\n\n\n\nIn dev mode, watch for file changes:\n@ApplicationScoped\n@RequiresProfile(\"dev\")\npublic class ConfigWatcher {\n    @ConfigProperty(name = \"orchestrator.config.watch\")\n    boolean watchEnabled;\n    \n    void onStart(@Observes StartupEvent event) {\n        if (watchEnabled) {\n            startWatching();\n        }\n    }\n    \n    private void startWatching() {\n        WatchService watchService = FileSystems.getDefault().newWatchService();\n        \n        Path graphsDir = Path.of(\"./graphs\");\n        Path tasksDir = Path.of(\"./tasks\");\n        \n        graphsDir.register(watchService, ENTRY_MODIFY, ENTRY_CREATE);\n        tasksDir.register(watchService, ENTRY_MODIFY, ENTRY_CREATE);\n        \n        new Thread(() -&gt; {\n            while (true) {\n                WatchKey key = watchService.take();\n                \n                for (WatchEvent&lt;?&gt; event : key.pollEvents()) {\n                    Path changed = (Path) event.context();\n                    logger.info(\"Config file changed: {}\", changed);\n                    \n                    // Reload\n                    if (changed.toString().endsWith(\".yaml\")) {\n                        reloadConfig(changed);\n                    }\n                }\n                \n                key.reset();\n            }\n        }).start();\n    }\n    \n    private void reloadConfig(Path file) {\n        try {\n            if (file.getParent().endsWith(\"graphs\")) {\n                graphLoader.reloadGraph(file);\n            } else if (file.getParent().endsWith(\"tasks\")) {\n                graphLoader.reloadTask(file);\n            }\n            logger.info(\"✓ Config reloaded: {}\", file);\n        } catch (Exception e) {\n            logger.error(\"✗ Failed to reload config: {}\", file, e);\n        }\n    }\n}\n\n\n\nExecute graphs without actually running commands. Perfect for testing configuration.\nEnable:\n# application-dev.yaml\norchestrator:\n  dev:\n    trial-run: true\nBehavior:\n@ApplicationScoped\npublic class TaskExecutor {\n    @ConfigProperty(name = \"orchestrator.dev.trial-run\")\n    boolean trialRun;\n    \n    public TaskResult execute(WorkMessage work) {\n        String fullCommand = buildCommandString(work);\n        \n        if (trialRun) {\n            // Log what would execute, but don't execute\n            logger.info(\"TRIAL RUN - Would execute: {}\", fullCommand);\n            logger.info(\"  Working dir: {}\", work.workingDir());\n            logger.info(\"  Environment: {}\", work.env());\n            logger.info(\"  Timeout: {}s\", work.timeoutSeconds());\n            \n            // Return fake success\n            return TaskResult.success(Map.of(\n                \"trial_run\", true,\n                \"command\", fullCommand\n            ));\n        }\n        \n        // Real execution\n        return executeCommand(work);\n    }\n}\nOutput:\n[INFO] Graph execution started: daily-etl (trial-run)\n[INFO] Task: load-market-data\n[INFO]   TRIAL RUN - Would execute: dbt run --models +market_data --vars batch_date:2025-10-17\n[INFO]   Working dir: /opt/dbt\n[INFO]   Environment: {DBT_PROFILES_DIR=/dbt/profiles, DBT_TARGET=dev}\n[INFO]   Timeout: 600s\n[INFO] Task: transform-data\n[INFO]   TRIAL RUN - Would execute: python /scripts/transform.py --date 2025-10-17\n[INFO]   Working dir: /opt/scripts\n[INFO]   Environment: {PYTHONUNBUFFERED=1}\n[INFO]   Timeout: 300s\n[INFO] Graph execution completed: daily-etl (trial-run) - 0.5s\n\n\n\nIn dev mode, enable in-browser editing of graphs and tasks.\nEnable:\norchestrator:\n  dev:\n    enable-editor: true\nUI Route: /editor\nFeatures: - Monaco Editor (VS Code’s editor) - YAML syntax highlighting - Real-time validation - Save to file - Auto-reload on save\nImplementation:\n&lt;!-- editor.html --&gt;\n&lt;div class=\"page-body\"&gt;\n  &lt;div class=\"container-xl\"&gt;\n    &lt;div class=\"row\"&gt;\n      &lt;div class=\"col-3\"&gt;\n        &lt;div class=\"card\"&gt;\n          &lt;div class=\"card-header\"&gt;\n            &lt;h3 class=\"card-title\"&gt;Files&lt;/h3&gt;\n          &lt;/div&gt;\n          &lt;div class=\"list-group list-group-flush\"&gt;\n            &lt;div class=\"list-group-item\"&gt;\n              &lt;div class=\"text-muted\"&gt;Graphs&lt;/div&gt;\n              {#for graph in graphs}\n              &lt;a href=\"#\" onclick=\"loadFile('graphs/{graph}.yaml')\" \n                 class=\"list-group-item list-group-item-action\"&gt;\n                {graph}.yaml\n              &lt;/a&gt;\n              {/for}\n            &lt;/div&gt;\n            &lt;div class=\"list-group-item\"&gt;\n              &lt;div class=\"text-muted\"&gt;Tasks&lt;/div&gt;\n              {#for task in tasks}\n              &lt;a href=\"#\" onclick=\"loadFile('tasks/{task}.yaml')\" \n                 class=\"list-group-item list-group-item-action\"&gt;\n                {task}.yaml\n              &lt;/a&gt;\n              {/for}\n            &lt;/div&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n      \n      &lt;div class=\"col-9\"&gt;\n        &lt;div class=\"card\"&gt;\n          &lt;div class=\"card-header\"&gt;\n            &lt;h3 class=\"card-title\" id=\"filename\"&gt;&lt;/h3&gt;\n            &lt;div class=\"card-actions\"&gt;\n              &lt;button class=\"btn btn-primary\" onclick=\"saveFile()\"&gt;\n                &lt;i class=\"ti ti-device-floppy\"&gt;&lt;/i&gt; Save\n              &lt;/button&gt;\n              &lt;button class=\"btn btn-secondary\" onclick=\"validateFile()\"&gt;\n                &lt;i class=\"ti ti-check\"&gt;&lt;/i&gt; Validate\n              &lt;/button&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n          &lt;div class=\"card-body p-0\"&gt;\n            &lt;div id=\"editor\" style=\"height: 600px;\"&gt;&lt;/div&gt;\n          &lt;/div&gt;\n          &lt;div class=\"card-footer\" id=\"validation-result\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;script src=\"https://cdn.jsdelivr.net/npm/monaco-editor@latest/min/vs/loader.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\nlet editor;\nlet currentFile;\n\nrequire.config({ \n  paths: { \n    vs: 'https://cdn.jsdelivr.net/npm/monaco-editor@latest/min/vs' \n  }\n});\n\nrequire(['vs/editor/editor.main'], function() {\n  editor = monaco.editor.create(document.getElementById('editor'), {\n    language: 'yaml',\n    theme: 'vs-dark',\n    automaticLayout: true,\n    minimap: { enabled: false }\n  });\n});\n\nasync function loadFile(path) {\n  currentFile = path;\n  document.getElementById('filename').textContent = path;\n  \n  const response = await fetch('/api/editor/load?path=' + path);\n  const content = await response.text();\n  \n  editor.setValue(content);\n}\n\nasync function saveFile() {\n  const content = editor.getValue();\n  \n  const response = await fetch('/api/editor/save', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      path: currentFile,\n      content: content\n    })\n  });\n  \n  if (response.ok) {\n    showValidation('✓ File saved successfully', 'success');\n  } else {\n    const error = await response.text();\n    showValidation('✗ Save failed: ' + error, 'danger');\n  }\n}\n\nasync function validateFile() {\n  const content = editor.getValue();\n  \n  const response = await fetch('/api/editor/validate', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      path: currentFile,\n      content: content\n    })\n  });\n  \n  const result = await response.json();\n  \n  if (result.valid) {\n    showValidation('✓ Valid YAML', 'success');\n  } else {\n    showValidation('✗ ' + result.error, 'danger');\n  }\n}\n\nfunction showValidation(message, type) {\n  const resultDiv = document.getElementById('validation-result');\n  resultDiv.innerHTML = `&lt;div class=\"alert alert-${type}\"&gt;${message}&lt;/div&gt;`;\n  setTimeout(() =&gt; {\n    resultDiv.innerHTML = '';\n  }, 3000);\n}\n&lt;/script&gt;\nAPI Endpoints:\n@Path(\"/api/editor\")\n@RequiresProfile(\"dev\")\npublic class EditorController {\n    @ConfigProperty(name = \"orchestrator.config.graphs\")\n    String graphsPath;\n    \n    @ConfigProperty(name = \"orchestrator.config.tasks\")\n    String tasksPath;\n    \n    @GET\n    @Path(\"/load\")\n    public String loadFile(@QueryParam(\"path\") String path) {\n        Path filePath = resolveSecurePath(path);\n        return Files.readString(filePath);\n    }\n    \n    @POST\n    @Path(\"/save\")\n    public Response saveFile(SaveRequest request) {\n        Path filePath = resolveSecurePath(request.path());\n        \n        // Validate first\n        ValidationResult validation = validator.validate(request.content());\n        if (!validation.isValid()) {\n            return Response.status(400).entity(validation.error()).build();\n        }\n        \n        // Write to file\n        Files.writeString(filePath, request.content());\n        \n        // Trigger reload\n        if (request.path().startsWith(\"graphs/\")) {\n            graphLoader.reloadGraph(filePath);\n        } else {\n            taskLoader.reloadTask(filePath);\n        }\n        \n        return Response.ok().build();\n    }\n    \n    @POST\n    @Path(\"/validate\")\n    public ValidationResult validateFile(ValidateRequest request) {\n        return validator.validate(request.content());\n    }\n    \n    private Path resolveSecurePath(String path) {\n        // Prevent directory traversal\n        Path base = Path.of(graphsPath).getParent();\n        Path resolved = base.resolve(path).normalize();\n        \n        if (!resolved.startsWith(base)) {\n            throw new SecurityException(\"Invalid path\");\n        }\n        \n        return resolved;\n    }\n}\n\n\n\n\n\n\n\nComponents: 1. Orchestrator (1 replica) - Stateless, can be restarted 2. Worker Pods (2-20 replicas, HPA) - Pull work from Hazelcast queue 3. Hazelcast Cluster (3 nodes, 1 per AZ) - State storage 4. ConfigMap - Graph and task YAML files 5. GCS - Event log storage\n\n\n\nStatefulSet:\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: hazelcast\n  namespace: orchestrator\nspec:\n  serviceName: hazelcast\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hazelcast\n  template:\n    metadata:\n      labels:\n        app: hazelcast\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - hazelcast\n            topologyKey: topology.kubernetes.io/zone\n      containers:\n      - name: hazelcast\n        image: hazelcast/hazelcast:5.3\n        env:\n        - name: JAVA_OPTS\n          value: \"-Xms2g -Xmx2g\"\n        ports:\n        - containerPort: 5701\n        resources:\n          requests:\n            memory: 2Gi\n            cpu: 500m\n          limits:\n            memory: 4Gi\n            cpu: 2000m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hazelcast\n  namespace: orchestrator\nspec:\n  clusterIP: None\n  selector:\n    app: hazelcast\n  ports:\n  - port: 5701\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orchestrator\n  namespace: orchestrator\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate  # Kill old before starting new\n  selector:\n    matchLabels:\n      app: orchestrator\n  template:\n    metadata:\n      labels:\n        app: orchestrator\n    spec:\n      serviceAccountName: orchestrator-sa\n      containers:\n      - name: orchestrator\n        image: gcr.io/project/orchestrator:latest\n        env:\n        - name: QUARKUS_PROFILE\n          value: prod\n        - name: ORCHESTRATOR_MODE\n          value: prod\n        - name: HAZELCAST_MEMBERS\n          value: \"hazelcast-0.hazelcast,hazelcast-1.hazelcast,hazelcast-2.hazelcast\"\n        - name: GCS_EVENTS_PATH\n          value: \"gs://my-bucket/orchestrator/events\"\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 1Gi\n            cpu: 500m\n          limits:\n            memory: 2Gi\n            cpu: 1000m\n        livenessProbe:\n          httpGet:\n            path: /q/health/live\n            port: 8080\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n          initialDelaySeconds: 10\n        volumeMounts:\n        - name: config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: workflow-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: orchestrator\n  namespace: orchestrator\nspec:\n  type: LoadBalancer\n  selector:\n    app: orchestrator\n  ports:\n  - port: 80\n    targetPort: 8080\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workers\n  namespace: orchestrator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      serviceAccountName: worker-sa\n      containers:\n      - name: worker\n        image: gcr.io/project/orchestrator-worker:latest\n        env:\n        - name: QUARKUS_PROFILE\n          value: prod\n        - name: WORKER_THREADS\n          value: \"16\"\n        - name: WORKER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: HAZELCAST_MEMBERS\n          value: \"hazelcast-0.hazelcast,hazelcast-1.hazelcast,hazelcast-2.hazelcast\"\n        resources:\n          requests:\n            memory: 2Gi\n            cpu: 1000m\n          limits:\n            memory: 4Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: workflow-config\n      terminationGracePeriodSeconds: 300  # Let tasks finish\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: workers-hpa\n  namespace: orchestrator\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: workers\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n      - type: Pods\n        value: 4\n        periodSeconds: 15\n\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-config\n  namespace: orchestrator\ndata:\n  # Tasks\n  load-market-data.yaml: |\n    name: load-market-data\n    global: true\n    key: \"load_market_${params.batch_date}_${params.region}\"\n    params:\n      batch_date:\n        type: string\n        required: true\n      region:\n        type: string\n        default: us\n    command: dbt\n    args:\n      - run\n      - --models\n      - +market_data\n    timeout: 600\n  \n  # Graphs\n  daily-etl.yaml: |\n    name: daily-etl\n    params:\n      batch_date:\n        type: string\n        default: \"${date.today()}\"\n    schedule: \"0 2 * * *\"\n    tasks:\n      - task: load-market-data\n        params:\n          batch_date: \"${params.batch_date}\"\n\n\n\nProcess: 1. Edit YAML files locally or in Git 2. Update ConfigMap 3. Restart orchestrator pod to reload config\n# Update ConfigMap from directory\nkubectl create configmap workflow-config \\\n  --from-file=graphs/ \\\n  --from-file=tasks/ \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n# Restart orchestrator to reload\nkubectl rollout restart deployment/orchestrator -n orchestrator\n\n# Wait for restart\nkubectl rollout status deployment/orchestrator -n orchestrator\nCI/CD Integration:\n# .github/workflows/deploy-workflows.yml\nname: Deploy Workflows\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'graphs/**'\n      - 'tasks/**'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Authenticate to GCP\n        uses: google-github-actions/auth@v1\n        with:\n          credentials_json: ${{ secrets.GCP_SA_KEY }}\n      \n      - name: Setup gcloud\n        uses: google-github-actions/setup-gcloud@v1\n      \n      - name: Get GKE credentials\n        run: |\n          gcloud container clusters get-credentials orchestrator \\\n            --region us-central1 --project my-project\n      \n      - name: Update ConfigMap\n        run: |\n          kubectl create configmap workflow-config \\\n            --from-file=graphs/ \\\n            --from-file=tasks/ \\\n            --dry-run=client -o yaml | kubectl apply -f -\n      \n      - name: Restart Orchestrator\n        run: |\n          kubectl rollout restart deployment/orchestrator -n orchestrator\n          kubectl rollout status deployment/orchestrator -n orchestrator\n\n\n\n\n\n\n\n\nFramework: Qute (Quarkus templating)\nCSS: Tabler.io (MIT license, comprehensive UI kit)\nIcons: Tabler Icons\nCharts: Chart.js\nDAG Visualization: dagre-d3\nGantt Charts: vis-timeline\nEditor: Monaco Editor (VS Code)\nInteractivity: Vanilla JavaScript + Server-Sent Events (SSE)\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│ Orchestrator                              [User] [Settings] │\n├─────┬───────────────────────────────────────────────────────┤\n│     │                                                       │\n│ Nav │  Page Content                                         │\n│     │                                                       │\n│     │                                                       │\n│Dash │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Grph │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Task │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Wrkr │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Evnt │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Sett │                                                       │\n│     │                                                       │\n└─────┴───────────────────────────────────────────────────────┘\n\n\n\n\n\nPurpose: Overview of system health and recent activity\nContent: - Stats cards: Active graphs, queued tasks, active workers, success rate - Recent executions table - Active graphs list with status - System health indicators\n\n\n\nPurpose: Browse all available graphs\nContent: - Searchable/filterable table - Columns: Name, Description, Last Run, Status, Actions - Actions: Execute, View, History\n\n\n\nPurpose: View graph execution in real-time\nTabs: 1. Topology - DAG visualization with color-coded nodes 2. Tasks - Table of all tasks with status, duration, logs link 3. Gantt - Timeline view of execution 4. Logs - Aggregated logs from all tasks\nActions: - Execute (with param form) - Pause/Resume - View History\n\n\n\nPurpose: View past executions\nContent: - Table of executions with filters (date range, status) - Click to view specific execution\n\n\n\nPurpose: View all global tasks and their current state\nContent: - Table of global tasks - Active executions (which graphs are using them) - Recent completions\n\n\n\nPurpose: Monitor worker health and utilization\nContent: - Table of workers with status, threads, active tasks - Worker metrics (CPU, memory if available) - Heartbeat indicators\n\n\n\nPurpose: View event log\nContent: - Searchable/filterable event stream - Real-time updates - Download events as JSONL\n\n\n\nPurpose: Edit graphs and tasks in browser\nContent: - File tree (graphs, tasks) - Monaco editor with YAML syntax highlighting - Save to file - Validation\n\n\n\nPurpose: System configuration\nContent: - Nuclear options (restart, hard reset) - System info - Configuration display\n\n\n\n\nUse Server-Sent Events (SSE) for live updates:\n@Path(\"/api/stream\")\npublic class StreamController {\n    @Inject\n    EventBus eventBus;\n    \n    @GET\n    @Path(\"/graphs/{id}\")\n    @Produces(MediaType.SERVER_SENT_EVENTS)\n    public Multi&lt;GraphEvent&gt; streamGraphEvents(@PathParam(\"id\") UUID graphId) {\n        return Multi.createFrom().emitter(emitter -&gt; {\n            Consumer&lt;Message&lt;Object&gt;&gt; handler = msg -&gt; {\n                GraphEvent event = convertToGraphEvent(msg.body());\n                emitter.emit(event);\n            };\n            \n            MessageConsumer&lt;Object&gt; consumer = eventBus.consumer(\"graph.\" + graphId);\n            consumer.handler(handler);\n            \n            emitter.onTermination(() -&gt; consumer.unregister());\n        });\n    }\n}\n// Client-side\nconst eventSource = new EventSource('/api/stream/graphs/123');\n\neventSource.addEventListener('task-status', (event) =&gt; {\n  const data = JSON.parse(event.data);\n  updateTaskRow(data.taskId, data.status);\n});\n\neventSource.addEventListener('graph-status', (event) =&gt; {\n  const data = JSON.parse(event.data);\n  updateGraphStatus(data.status);\n});\n\n\n\n\n\n\n\n\n\n// Graph definitions (loaded from YAML)\nIMap&lt;String, Graph&gt; graphDefinitions;\n\n// Task definitions (loaded from YAML)\nIMap&lt;String, Task&gt; taskDefinitions;\n\n// Graph executions (current state)\nIMap&lt;UUID, GraphExecution&gt; graphExecutions;\n\n// Task executions (current state)\nIMap&lt;UUID, TaskExecution&gt; taskExecutions;\n\n// Global task executions (deduplicated)\nIMap&lt;TaskExecutionKey, GlobalTaskExecution&gt; globalTasks;\n\n// Worker registry\nIMap&lt;String, Worker&gt; workers;\n\n\n\n// Work queue (tasks waiting for workers)\nIQueue&lt;WorkMessage&gt; workQueue;\n\n\n\n\nFile Structure:\nevents/\n├── 2025-10-15.jsonl\n├── 2025-10-16.jsonl\n└── 2025-10-17.jsonl\nEvent Types:\n{\"event_type\":\"GRAPH_STARTED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"graph_execution_id\":\"uuid\",\"graph_name\":\"daily-etl\",\"triggered_by\":\"schedule\",\"params\":{\"batch_date\":\"2025-10-17\"}}\n\n{\"event_type\":\"TASK_QUEUED\",\"timestamp\":\"2025-10-17T10:00:01Z\",\"task_execution_id\":\"uuid\",\"task_name\":\"load-data\",\"graph_execution_id\":\"uuid\"}\n\n{\"event_type\":\"TASK_STARTED\",\"timestamp\":\"2025-10-17T10:00:05Z\",\"task_execution_id\":\"uuid\",\"worker_id\":\"worker-1\",\"thread\":\"worker-1-thread-3\"}\n\n{\"event_type\":\"TASK_COMPLETED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"task_execution_id\":\"uuid\",\"duration_seconds\":295,\"result\":{\"rows_loaded\":150000}}\n\n{\"event_type\":\"TASK_FAILED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"task_execution_id\":\"uuid\",\"error\":\"Connection timeout\",\"attempt\":1}\n\n{\"event_type\":\"GRAPH_COMPLETED\",\"timestamp\":\"2025-10-17T10:30:00Z\",\"graph_execution_id\":\"uuid\",\"status\":\"COMPLETED\",\"duration_seconds\":1800}\n\n{\"event_type\":\"GLOBAL_TASK_STARTED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"task_name\":\"load-market-data\",\"resolved_key\":\"load_market_2025-10-17_us\",\"params\":{\"batch_date\":\"2025-10-17\",\"region\":\"us\"},\"initiated_by_graph\":\"uuid\"}\n\n{\"event_type\":\"GLOBAL_TASK_LINKED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"resolved_key\":\"load_market_2025-10-17_us\",\"linked_graph\":\"uuid\"}\n\n{\"event_type\":\"GLOBAL_TASK_COMPLETED\",\"timestamp\":\"2025-10-17T10:15:00Z\",\"resolved_key\":\"load_market_2025-10-17_us\",\"result\":{\"rows\":1500000}}\n\n{\"event_type\":\"WORKER_HEARTBEAT\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"worker_id\":\"worker-1\",\"active_threads\":3,\"total_threads\":16}\n\n{\"event_type\":\"WORKER_DIED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"worker_id\":\"worker-2\"}\n\n{\"event_type\":\"CLUSTER_RESTART\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"initiated_by\":\"admin@company.com\"}\n\n{\"event_type\":\"CLUSTER_RESET\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"initiated_by\":\"admin@company.com\"}\n\n\n\n@ApplicationScoped\npublic class StateRecoveryService {\n    void onStart(@Observes StartupEvent event) {\n        // Find last checkpoint or reset event\n        Instant recoverySince = findRecoveryPoint();\n        \n        // Read events since recovery point\n        Stream&lt;Event&gt; events = eventStore.readSince(recoverySince);\n        \n        // Replay events to rebuild state\n        events.forEach(this::replayEvent);\n        \n        // Clean up stale state\n        cleanupStaleExecutions();\n    }\n    \n    private Instant findRecoveryPoint() {\n        // Look for CLUSTER_RESET events\n        Optional&lt;Event&gt; reset = eventStore.findLastEvent(\"CLUSTER_RESET\");\n        if (reset.isPresent()) {\n            return reset.get().timestamp();\n        }\n        \n        // Default: recover last 24 hours\n        return Instant.now().minus(24, ChronoUnit.HOURS);\n    }\n}\n\n\n\n\n\n\n\n\n\n@ApplicationScoped\npublic class GraphLoader {\n    @Inject\n    ExpressionEvaluator expressionEvaluator;\n    @Inject\n    GraphValidator graphValidator;\n    @Inject\n    HazelcastInstance hazelcast;\n    \n    @ConfigProperty(name = \"orchestrator.config.graphs\")\n    String graphsPath;\n    \n    @ConfigProperty(name = \"orchestrator.config.tasks\")\n    String tasksPath;\n    \n    private IMap&lt;String, Graph&gt; graphDefinitions;\n    private IMap&lt;String, Task&gt; taskDefinitions;\n    \n    void onStart(@Observes StartupEvent event) {\n        graphDefinitions = hazelcast.getMap(\"graph-definitions\");\n        taskDefinitions = hazelcast.getMap(\"task-definitions\");\n        \n        loadTasks();\n        loadGraphs();\n    }\n    \n    private void loadTasks() {\n        try (Stream&lt;Path&gt; paths = Files.walk(Path.of(tasksPath))) {\n            paths.filter(p -&gt; p.toString().endsWith(\".yaml\"))\n                 .forEach(this::loadTask);\n        }\n    }\n    \n    private void loadTask(Path path) {\n        String yaml = Files.readString(path);\n        Task task = parseTask(yaml);\n        \n        // Validate\n        validator.validateTask(task);\n        \n        // Store\n        taskDefinitions.put(task.name(), task);\n        \n        logger.info(\"Loaded task: {}\", task.name());\n    }\n    \n    private void loadGraphs() {\n        try (Stream&lt;Path&gt; paths = Files.walk(Path.of(graphsPath))) {\n            paths.filter(p -&gt; p.toString().endsWith(\".yaml\"))\n                 .forEach(this::loadGraph);\n        }\n    }\n    \n    private void loadGraph(Path path) {\n        String yaml = Files.readString(path);\n        Graph graph = parseGraph(yaml);\n        \n        // Validate\n        graphValidator.validateGraph(graph);\n        \n        // Store\n        graphDefinitions.put(graph.name(), graph);\n        \n        logger.info(\"Loaded graph: {} ({} tasks)\", \n            graph.name(), graph.tasks().size());\n    }\n}\n\n\n\n@ApplicationScoped\npublic class GraphEvaluator {\n    @Inject\n    EventBus eventBus;\n    @Inject\n    JGraphTService jGraphTService;\n    @Inject\n    HazelcastInstance hazelcast;\n    \n    private IMap&lt;UUID, GraphExecution&gt; graphExecutions;\n    private IMap&lt;UUID, TaskExecution&gt; taskExecutions;\n    private IMap&lt;TaskExecutionKey, GlobalTaskExecution&gt; globalTasks;\n    \n    @PostConstruct\n    void init() {\n        graphExecutions = hazelcast.getMap(\"graph-executions\");\n        taskExecutions = hazelcast.getMap(\"task-executions\");\n        globalTasks = hazelcast.getMap(\"global-tasks\");\n    }\n    \n    @ConsumeEvent(\"task.completed\")\n    @ConsumeEvent(\"task.failed\")\n    public void onTaskStatusChange(TaskEvent event) {\n        // Update task execution\n        TaskExecution te = taskExecutions.get(event.executionId());\n        te = te.withStatus(event.status());\n        taskExecutions.put(event.executionId(), te);\n        \n        // Find graph execution(s) to re-evaluate\n        if (te.isGlobal()) {\n            // Global task - notify all linked graphs\n            GlobalTaskExecution gte = globalTasks.get(te.globalKey());\n            gte.linkedGraphExecutions().forEach(graphExecId -&gt; \n                eventBus.publish(\"graph.evaluate\", graphExecId)\n            );\n        } else {\n            // Regular task - notify its graph\n            eventBus.publish(\"graph.evaluate\", te.graphExecutionId());\n        }\n    }\n    \n    @ConsumeEvent(\"graph.evaluate\")\n    public void evaluate(UUID graphExecutionId) {\n        GraphExecution exec = graphExecutions.get(graphExecutionId);\n        \n        // Build DAG\n        DirectedAcyclicGraph&lt;TaskNode, DefaultEdge&gt; dag = \n            jGraphTService.buildDAG(exec.graph());\n        \n        // Get current state\n        Map&lt;TaskNode, TaskStatus&gt; state = getCurrentState(graphExecutionId);\n        \n        // Find ready tasks\n        Set&lt;TaskNode&gt; ready = jGraphTService.findReadyTasks(dag, state);\n        \n        // Schedule each ready task\n        ready.forEach(task -&gt; scheduleTask(graphExecutionId, task));\n        \n        // Update graph status\n        updateGraphStatus(graphExecutionId, state);\n    }\n    \n    private void scheduleTask(UUID graphExecutionId, TaskNode node) {\n        if (node.isGlobal()) {\n            scheduleGlobalTask(graphExecutionId, node);\n        } else {\n            scheduleRegularTask(graphExecutionId, node);\n        }\n    }\n}\n\n\n\n@ApplicationScoped\npublic class WorkerPool {\n    @Inject\n    HazelcastInstance hazelcast;\n    @Inject\n    TaskExecutor taskExecutor;\n    @Inject\n    EventLogger eventLogger;\n    \n    @ConfigProperty(name = \"worker.threads\")\n    int workerThreads;\n    \n    @ConfigProperty(name = \"worker.id\")\n    String workerId;\n    \n    private IQueue&lt;WorkMessage&gt; workQueue;\n    private ExecutorService executorService;\n    private volatile boolean running = false;\n    \n    @PostConstruct\n    void init() {\n        workQueue = hazelcast.getQueue(\"work-queue\");\n    }\n    \n    public void start(int threads) {\n        this.workerThreads = threads;\n        this.running = true;\n        \n        // Create thread pool\n        this.executorService = Executors.newFixedThreadPool(\n            workerThreads,\n            new ThreadFactory() {\n                private final AtomicInteger counter = new AtomicInteger(0);\n                \n                @Override\n                public Thread newThread(Runnable r) {\n                    Thread t = new Thread(r);\n                    t.setName(workerId + \"-thread-\" + counter.incrementAndGet());\n                    return t;\n                }\n            }\n        );\n        \n        // Start worker threads\n        for (int i = 0; i &lt; workerThreads; i++) {\n            executorService.submit(this::workerLoop);\n        }\n        \n        // Start heartbeat\n        startHeartbeat();\n        \n        logger.info(\"Worker pool started: {} threads\", workerThreads);\n    }\n    \n    private void workerLoop() {\n        String threadName = Thread.currentThread().getName();\n        \n        while (running) {\n            try {\n                // Pull work (blocking with timeout)\n                WorkMessage work = workQueue.poll(5, TimeUnit.SECONDS);\n                \n                if (work == null) {\n                    continue;  // Timeout, try again\n                }\n                \n                executeWork(work, threadName);\n                \n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n                break;\n            } catch (Exception e) {\n                logger.error(\"[{}] Error in worker loop\", threadName, e);\n            }\n        }\n    }\n    \n    private void executeWork(WorkMessage work, String threadName) {\n        logger.info(\"[{}] Executing: {}\", threadName, work.taskName());\n        \n        eventLogger.logTaskStarted(work.executionId(), workerId, threadName);\n        \n        Instant start = Instant.now();\n        \n        try {\n            TaskResult result = taskExecutor.execute(work);\n            Duration duration = Duration.between(start, Instant.now());\n            \n            if (result.isSuccess()) {\n                logger.info(\"[{}] Completed: {} ({})\", \n                    threadName, work.taskName(), duration);\n                \n                eventLogger.logTaskCompleted(\n                    work.executionId(), result.data(), duration);\n            } else {\n                logger.error(\"[{}] Failed: {}\", threadName, work.taskName());\n                \n                eventLogger.logTaskFailed(\n                    work.executionId(), result.error(), work.attempt());\n            }\n            \n        } catch (Exception e) {\n            logger.error(\"[{}] Exception: {}\", threadName, work.taskName(), e);\n            \n            eventLogger.logTaskFailed(\n                work.executionId(), e.getMessage(), work.attempt());\n        }\n    }\n}\n\n\n\n@ApplicationScoped\npublic class TaskExecutor {\n    @ConfigProperty(name = \"orchestrator.dev.trial-run\")\n    boolean trialRun;\n    \n    @Inject\n    ExpressionEvaluator expressionEvaluator;\n    \n    public TaskResult execute(WorkMessage work) {\n        // Evaluate all expressions\n        String command = expressionEvaluator.evaluate(\n            work.command(), work.context());\n        \n        List&lt;String&gt; args = work.args().stream()\n            .map(arg -&gt; expressionEvaluator.evaluate(arg, work.context()))\n            .toList();\n        \n        Map&lt;String, String&gt; env = work.env().entrySet().stream()\n            .collect(Collectors.toMap(\n                Map.Entry::getKey,\n                e -&gt; expressionEvaluator.evaluate(e.getValue(), work.context())\n            ));\n        \n        // Build full command\n        List&lt;String&gt; fullCommand = new ArrayList&lt;&gt;();\n        fullCommand.add(command);\n        fullCommand.addAll(args);\n        \n        // Trial run mode\n        if (trialRun) {\n            return executeTrialRun(fullCommand, env, work);\n        }\n        \n        // Real execution\n        return executeCommand(fullCommand, env, work);\n    }\n    \n    private TaskResult executeTrialRun(\n        List&lt;String&gt; command, \n        Map&lt;String, String&gt; env,\n        WorkMessage work\n    ) {\n        String commandStr = String.join(\" \", command);\n        \n        logger.info(\"═══════════════════════════════════════\");\n        logger.info(\"TRIAL RUN - Would execute:\");\n        logger.info(\"  Command: {}\", commandStr);\n        logger.info(\"  Timeout: {}s\", work.timeoutSeconds());\n        logger.info(\"  Environment:\");\n        env.forEach((k, v) -&gt; logger.info(\"    {}={}\", k, v));\n        logger.info(\"═══════════════════════════════════════\");\n        \n        // Return fake success\n        return TaskResult.success(Map.of(\n            \"trial_run\", true,\n            \"command\", commandStr,\n            \"would_timeout\", work.timeoutSeconds()\n        ));\n    }\n    \n    private TaskResult executeCommand(\n        List&lt;String&gt; command,\n        Map&lt;String, String&gt; env,\n        WorkMessage work\n    ) {\n        try {\n            ProcessBuilder pb = new ProcessBuilder(command);\n            \n            // Set environment\n            pb.environment().putAll(env);\n            \n            // Redirect stderr to stdout\n            pb.redirectErrorStream(true);\n            \n            // Start process\n            Process process = pb.start();\n            \n            // Capture output\n            StringBuilder output = new StringBuilder();\n            try (BufferedReader reader = new BufferedReader(\n                    new InputStreamReader(process.getInputStream()))) {\n                String line;\n                while ((line = reader.readLine()) != null) {\n                    output.append(line).append(\"\\n\");\n                    logger.info(\"[TASK OUTPUT] {}\", line);\n                }\n            }\n            \n            // Wait for completion with timeout\n            boolean completed = process.waitFor(\n                work.timeoutSeconds(), \n                TimeUnit.SECONDS\n            );\n            \n            if (!completed) {\n                process.destroyForcibly();\n                return TaskResult.failure(\n                    \"Task timed out after \" + work.timeoutSeconds() + \"s\");\n            }\n            \n            int exitCode = process.exitValue();\n            \n            if (exitCode == 0) {\n                // Try to parse last line as JSON for downstream tasks\n                Map&lt;String, Object&gt; result = tryParseJsonOutput(\n                    output.toString());\n                \n                return TaskResult.success(result);\n            } else {\n                return TaskResult.failure(\n                    \"Task exited with code \" + exitCode + \"\\n\" + \n                    output.toString());\n            }\n            \n        } catch (IOException e) {\n            return TaskResult.failure(\n                \"Failed to start process: \" + e.getMessage());\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            return TaskResult.failure(\"Task interrupted\");\n        }\n    }\n    \n    private Map&lt;String, Object&gt; tryParseJsonOutput(String output) {\n        try {\n            String[] lines = output.split(\"\\n\");\n            if (lines.length == 0) {\n                return Map.of(\"output\", output);\n            }\n            \n            String lastLine = lines[lines.length - 1].trim();\n            \n            if (lastLine.startsWith(\"{\") && lastLine.endsWith(\"}\")) {\n                ObjectMapper mapper = new ObjectMapper();\n                return mapper.readValue(lastLine, \n                    new TypeReference&lt;Map&lt;String, Object&gt;&gt;() {});\n            }\n        } catch (Exception e) {\n            // Not JSON, that's fine\n        }\n        \n        return Map.of(\"output\", output);\n    }\n}\n\n\n\n@ApplicationScoped\npublic class EventStore {\n    @ConfigProperty(name = \"orchestrator.storage.events\")\n    String eventsPath;\n    \n    @Inject\n    StorageAdapter storageAdapter;\n    \n    private final DateTimeFormatter dateFormat = \n        DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\n    \n    public void append(Event event) {\n        String date = dateFormat.format(\n            LocalDate.ofInstant(event.timestamp(), ZoneOffset.UTC));\n        \n        String filename = date + \".jsonl\";\n        String path = eventsPath + \"/\" + filename;\n        \n        String json = toJson(event) + \"\\n\";\n        \n        storageAdapter.append(path, json);\n    }\n    \n    public Stream&lt;Event&gt; readSince(Instant since) {\n        LocalDate sinceDate = LocalDate.ofInstant(since, ZoneOffset.UTC);\n        LocalDate today = LocalDate.now();\n        \n        return sinceDate.datesUntil(today.plusDays(1))\n            .flatMap(date -&gt; {\n                String filename = dateFormat.format(date) + \".jsonl\";\n                String path = eventsPath + \"/\" + filename;\n                \n                try {\n                    return storageAdapter.readLines(path)\n                        .map(this::parseEvent)\n                        .filter(e -&gt; e.timestamp().isAfter(since));\n                } catch (IOException e) {\n                    logger.warn(\"Could not read events from {}\", path);\n                    return Stream.empty();\n                }\n            });\n    }\n    \n    public Optional&lt;Event&gt; findLastEvent(String eventType) {\n        // Search backwards from today\n        LocalDate date = LocalDate.now();\n        \n        for (int i = 0; i &lt; 30; i++) {  // Search last 30 days\n            String filename = dateFormat.format(date) + \".jsonl\";\n            String path = eventsPath + \"/\" + filename;\n            \n            try {\n                List&lt;Event&gt; events = storageAdapter.readLines(path)\n                    .map(this::parseEvent)\n                    .filter(e -&gt; e.eventType().equals(eventType))\n                    .toList();\n                \n                if (!events.isEmpty()) {\n                    return Optional.of(events.get(events.size() - 1));\n                }\n            } catch (IOException e) {\n                // File doesn't exist, continue\n            }\n            \n            date = date.minusDays(1);\n        }\n        \n        return Optional.empty();\n    }\n    \n    private String toJson(Event event) {\n        try {\n            ObjectMapper mapper = new ObjectMapper();\n            return mapper.writeValueAsString(event);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(\"Failed to serialize event\", e);\n        }\n    }\n    \n    private Event parseEvent(String json) {\n        try {\n            ObjectMapper mapper = new ObjectMapper();\n            return mapper.readValue(json, Event.class);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(\"Failed to parse event\", e);\n        }\n    }\n}\n\n\n\npublic interface StorageAdapter {\n    void append(String path, String content);\n    Stream&lt;String&gt; readLines(String path) throws IOException;\n}\n\n@ApplicationScoped\npublic class StorageAdapterFactory {\n    public StorageAdapter create(String path) {\n        if (path.startsWith(\"gs://\")) {\n            return new GcsStorageAdapter();\n        } else if (path.startsWith(\"s3://\")) {\n            return new S3StorageAdapter();\n        } else if (path.startsWith(\"file://\") || path.startsWith(\"./\")) {\n            return new LocalFilesystemAdapter();\n        } else {\n            throw new IllegalArgumentException(\n                \"Unsupported storage path: \" + path);\n        }\n    }\n}\n\n@ApplicationScoped\npublic class LocalFilesystemAdapter implements StorageAdapter {\n    @Override\n    public void append(String path, String content) {\n        Path filePath = Path.of(path.replace(\"file://\", \"\"));\n        \n        // Create directory if needed\n        Files.createDirectories(filePath.getParent());\n        \n        // Append to file\n        Files.writeString(filePath, content, \n            StandardOpenOption.CREATE, \n            StandardOpenOption.APPEND);\n    }\n    \n    @Override\n    public Stream&lt;String&gt; readLines(String path) throws IOException {\n        Path filePath = Path.of(path.replace(\"file://\", \"\"));\n        return Files.lines(filePath);\n    }\n}\n\n@ApplicationScoped\npublic class GcsStorageAdapter implements StorageAdapter {\n    @Inject\n    Storage storage;\n    \n    @Override\n    public void append(String path, String content) {\n        // Parse gs://bucket/path\n        String pathWithoutScheme = path.substring(\"gs://\".length());\n        String[] parts = pathWithoutScheme.split(\"/\", 2);\n        String bucket = parts[0];\n        String objectPath = parts[1];\n        \n        BlobId blobId = BlobId.of(bucket, objectPath);\n        BlobInfo blobInfo = BlobInfo.newBuilder(blobId).build();\n        \n        // Check if exists\n        Blob blob = storage.get(blobId);\n        \n        if (blob != null) {\n            // Append to existing\n            byte[] existing = blob.getContent();\n            byte[] combined = concat(existing, content.getBytes());\n            storage.create(blobInfo, combined);\n        } else {\n            // Create new\n            storage.create(blobInfo, content.getBytes());\n        }\n    }\n    \n    @Override\n    public Stream&lt;String&gt; readLines(String path) throws IOException {\n        String pathWithoutScheme = path.substring(\"gs://\".length());\n        String[] parts = pathWithoutScheme.split(\"/\", 2);\n        String bucket = parts[0];\n        String objectPath = parts[1];\n        \n        BlobId blobId = BlobId.of(bucket, objectPath);\n        Blob blob = storage.get(blobId);\n        \n        if (blob == null) {\n            throw new FileNotFoundException(path);\n        }\n        \n        String content = new String(blob.getContent());\n        return content.lines();\n    }\n}\n\n\n\n\n\n\n\n\nRepository Tests:\n@QuarkusTest\nclass GraphRepositoryTest {\n    @Inject\n    GraphRepository graphRepo;\n    \n    @Test\n    void shouldLoadGraphFromYaml() {\n        String yaml = \"\"\"\n            name: test-graph\n            tasks:\n              - name: task1\n                command: echo\n                args:\n                  - hello\n            \"\"\";\n        \n        Graph graph = graphRepo.parseYaml(yaml);\n        \n        assertThat(graph.name()).isEqualTo(\"test-graph\");\n        assertThat(graph.tasks()).hasSize(1);\n    }\n}\nExpression Evaluator Tests:\n@QuarkusTest\nclass ExpressionEvaluatorTest {\n    @Inject\n    ExpressionEvaluator evaluator;\n    \n    @Test\n    void shouldEvaluateSimpleExpression() {\n        Context ctx = new Context(\n            Map.of(\"region\", \"us\"),\n            Map.of(),\n            Map.of()\n        );\n        \n        String result = evaluator.evaluate(\"${params.region}\", ctx);\n        \n        assertThat(result).isEqualTo(\"us\");\n    }\n    \n    @Test\n    void shouldEvaluateConditional() {\n        Context ctx = new Context(\n            Map.of(\"env\", \"prod\"),\n            Map.of(),\n            Map.of()\n        );\n        \n        String result = evaluator.evaluate(\n            \"${params.env == 'prod' ? 'production' : 'development'}\", \n            ctx\n        );\n        \n        assertThat(result).isEqualTo(\"production\");\n    }\n}\nJGraphT Service Tests:\n@QuarkusTest\nclass JGraphTServiceTest {\n    @Inject\n    JGraphTService jGraphTService;\n    \n    @Test\n    void shouldDetectCycle() {\n        Graph graph = createGraphWithCycle();\n        \n        assertThatThrownBy(() -&gt; jGraphTService.buildDAG(graph))\n            .isInstanceOf(CycleFoundException.class);\n    }\n    \n    @Test\n    void shouldFindReadyTasks() {\n        // A -&gt; B -&gt; C\n        Graph graph = createLinearGraph();\n        DirectedAcyclicGraph&lt;TaskNode, DefaultEdge&gt; dag = \n            jGraphTService.buildDAG(graph);\n        \n        Map&lt;TaskNode, TaskStatus&gt; state = Map.of(\n            taskA, TaskStatus.COMPLETED,\n            taskB, TaskStatus.PENDING,\n            taskC, TaskStatus.PENDING\n        );\n        \n        Set&lt;TaskNode&gt; ready = jGraphTService.findReadyTasks(dag, state);\n        \n        assertThat(ready).containsOnly(taskB);\n    }\n}\n\n\n\nFull Graph Execution:\n@QuarkusTest\n@TestProfile(IntegrationTestProfile.class)\nclass GraphExecutionIntegrationTest {\n    @Inject\n    GraphEvaluator evaluator;\n    @Inject\n    GraphRepository graphRepo;\n    @Inject\n    WorkerPool workerPool;\n    \n    @BeforeEach\n    void setup() {\n        // Start embedded worker\n        workerPool.start(2);\n    }\n    \n    @Test\n    void shouldExecuteSimpleGraph() {\n        // Load test graph\n        Graph graph = graphRepo.loadFromFile(\"test-graphs/simple.yaml\");\n        \n        // Create execution\n        GraphExecution exec = graphExecRepo.create(\n            graph.id(),\n            Map.of(\"date\", \"2025-10-17\"),\n            \"test\"\n        );\n        \n        // Trigger evaluation\n        evaluator.evaluate(exec.id());\n        \n        // Wait for completion\n        await().atMost(30, SECONDS).until(() -&gt; {\n            GraphExecution updated = graphExecRepo.findById(exec.id());\n            return updated.status() == GraphStatus.COMPLETED;\n        });\n        \n        // Verify all tasks completed\n        List&lt;TaskExecution&gt; tasks = \n            taskExecRepo.findByGraphExecution(exec.id());\n        \n        assertThat(tasks).allMatch(te -&gt; \n            te.status() == TaskStatus.COMPLETED);\n    }\n    \n    @Test\n    void shouldHandleGlobalTaskDeduplication() {\n        // Create two graphs that use same global task\n        Graph graph1 = graphRepo.loadFromFile(\"test-graphs/with-global-1.yaml\");\n        Graph graph2 = graphRepo.loadFromFile(\"test-graphs/with-global-2.yaml\");\n        \n        Map&lt;String, Object&gt; params = Map.of(\"date\", \"2025-10-17\");\n        \n        // Start both executions\n        GraphExecution exec1 = graphExecRepo.create(\n            graph1.id(), params, \"test\");\n        GraphExecution exec2 = graphExecRepo.create(\n            graph2.id(), params, \"test\");\n        \n        evaluator.evaluate(exec1.id());\n        evaluator.evaluate(exec2.id());\n        \n        // Wait for completion\n        await().atMost(30, SECONDS).until(() -&gt; {\n            GraphExecution e1 = graphExecRepo.findById(exec1.id());\n            GraphExecution e2 = graphExecRepo.findById(exec2.id());\n            return e1.status() == GraphStatus.COMPLETED &&\n                   e2.status() == GraphStatus.COMPLETED;\n        });\n        \n        // Verify only ONE global task execution occurred\n        List&lt;GlobalTaskExecution&gt; globalExecs = \n            globalTaskRepo.findByResolvedKey(\"load_data_2025-10-17\");\n        \n        assertThat(globalExecs).hasSize(1);\n        assertThat(globalExecs.get(0).linkedGraphExecutions())\n            .containsExactlyInAnyOrder(exec1.id(), exec2.id());\n    }\n}\nTrial Run Test:\n@QuarkusTest\n@TestProfile(TrialRunTestProfile.class)\nclass TrialRunTest {\n    @Inject\n    TaskExecutor taskExecutor;\n    \n    @Test\n    void shouldLogCommandWithoutExecuting() {\n        WorkMessage work = new WorkMessage(\n            UUID.randomUUID(),\n            \"test-task\",\n            \"python\",\n            List.of(\"script.py\", \"--dangerous\"),\n            Map.of(\"API_KEY\", \"secret\"),\n            600\n        );\n        \n        TaskResult result = taskExecutor.execute(work);\n        \n        // Should succeed without actually running\n        assertThat(result.isSuccess()).isTrue();\n        assertThat(result.data()).containsEntry(\"trial_run\", true);\n        assertThat(result.data()).containsEntry(\"command\", \n            \"python script.py --dangerous\");\n        \n        // Verify nothing was actually executed (check logs)\n    }\n}\n\n\n\nUse Testcontainers for full stack:\n@QuarkusTest\n@TestProfile(E2ETestProfile.class)\nclass EndToEndTest {\n    @Container\n    static HazelcastContainer hazelcast = \n        new HazelcastContainer(\"hazelcast/hazelcast:5.3\");\n    \n    @Test\n    void shouldExecuteGraphEndToEnd() {\n        // Deploy graph via API\n        given()\n            .contentType(\"application/yaml\")\n            .body(loadTestGraph())\n        .when()\n            .post(\"/api/graphs\")\n        .then()\n            .statusCode(201);\n        \n        // Trigger execution\n        String executionId = given()\n            .contentType(\"application/json\")\n            .body(Map.of(\"params\", Map.of(\"date\", \"2025-10-17\")))\n        .when()\n            .post(\"/api/graphs/test-graph/execute\")\n        .then()\n            .statusCode(200)\n            .extract()\n            .path(\"executionId\");\n        \n        // Poll for completion\n        await().atMost(60, SECONDS).until(() -&gt; {\n            String status = given()\n                .when()\n                    .get(\"/api/graphs/executions/\" + executionId)\n                .then()\n                    .statusCode(200)\n                    .extract()\n                    .path(\"status\");\n            \n            return \"COMPLETED\".equals(status);\n        });\n        \n        // Verify events logged\n        String events = given()\n            .when()\n                .get(\"/api/events?execution=\" + executionId)\n            .then()\n                .statusCode(200)\n                .extract()\n                .asString();\n        \n        assertThat(events).contains(\"GRAPH_STARTED\");\n        assertThat(events).contains(\"TASK_COMPLETED\");\n        assertThat(events).contains(\"GRAPH_COMPLETED\");\n    }\n}\n\n\n\n\n\n\n\nPrerequisites: - Java 21+ - Maven or Gradle\nSteps:\n\nClone repository\n\ngit clone https://github.com/yourorg/orchestrator.git\ncd orchestrator\n\nBuild\n\n./mvnw clean package\n\nCreate config directories\n\nmkdir -p graphs tasks data/events\n\nCreate sample graph\n\ncat &gt; graphs/hello-world.yaml &lt;&lt;EOF\nname: hello-world\ntasks:\n  - name: say-hello\n    command: echo\n    args:\n      - \"Hello, World!\"\nEOF\n\nRun\n\njava -jar target/quarkus-app/quarkus-run.jar\n\nOpen browser\n\nopen http://localhost:8080\n\n\n\nBuild:\ndocker build -t orchestrator:latest .\nRun:\ndocker run -p 8080:8080 \\\n  -v $(pwd)/graphs:/config/graphs \\\n  -v $(pwd)/tasks:/config/tasks \\\n  -v $(pwd)/data:/data \\\n  orchestrator:latest\n\n\n\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  hazelcast:\n    image: hazelcast/hazelcast:5.3\n    environment:\n      JAVA_OPTS: \"-Xms512m -Xmx512m\"\n    ports:\n      - \"5701:5701\"\n  \n  orchestrator:\n    build: .\n    environment:\n      ORCHESTRATOR_MODE: prod\n      HAZELCAST_MEMBERS: hazelcast:5701\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./graphs:/config/graphs:ro\n      - ./tasks:/config/tasks:ro\n      - ./data:/data\n    depends_on:\n      - hazelcast\n  \n  worker:\n    build: .\n    command: [\"worker\"]\n    environment:\n      WORKER_THREADS: 8\n      HAZELCAST_MEMBERS: hazelcast:5701\n    volumes:\n      - ./graphs:/config/graphs:ro\n      - ./tasks:/config/tasks:ro\n    depends_on:\n      - hazelcast\n    deploy:\n      replicas: 2\n\n\n\nPrerequisites: - GKE cluster - kubectl configured - Container images pushed to GCR\nDeploy Hazelcast:\nkubectl apply -f k8s/hazelcast-statefulset.yaml\nCreate ConfigMap:\nkubectl create configmap workflow-config \\\n  --from-file=graphs/ \\\n  --from-file=tasks/ \\\n  -n orchestrator\nDeploy Orchestrator:\nkubectl apply -f k8s/orchestrator-deployment.yaml\nDeploy Workers:\nkubectl apply -f k8s/worker-deployment.yaml\nVerify:\nkubectl get pods -n orchestrator\nkubectl logs -n orchestrator deployment/orchestrator\nAccess UI:\nkubectl port-forward -n orchestrator svc/orchestrator 8080:80\nopen http://localhost:8080\n\n\n\n\n\n\n\nA lightweight, portable, event-driven workflow orchestrator that:\n\nRuns anywhere - Single binary, no dependencies\nUses simple YAML - No Python code required\nDeduplicates work - Global tasks run once, notify many\nScales efficiently - Multi-threaded workers for I/O workloads\nHas great DX - Dev mode, trial run, hot reload, web editor\nIs observable - Events in files, metrics, real-time UI\nGets out of your way - Simple, fast, focused\n\n\n\n\n✅ Global Tasks - Run once per parameter set, notify all graphs ✅ Parameter Scoping - Clean override hierarchy ✅ JEXL Expressions - Powerful, safe templating ✅ Single Process Dev Mode - Everything in one process ✅ Trial Run - Test without executing ✅ Web Editor - Edit YAML in browser (dev only) ✅ File-Based Events - JSONL files, not databases ✅ Multi-Threaded Workers - Efficient for GCP workloads ✅ Tabler UI - Professional, polished interface ✅ Hazelcast State - Fast, distributed state management\n\n\n\n\nLanguage: Java 21\nFramework: Quarkus\nState: Hazelcast (embedded in dev, clustered in prod)\nEvents: JSONL files (local/GCS/S3)\nUI: Qute + Tabler CSS + Monaco Editor\nExpressions: Apache Commons JEXL\nDAG: JGraphT\nDeployment: Single binary or GKE\n\n\n\n\n\nCreate GitHub repository\nGenerate feature issues from this design\nSetup CI/CD pipeline (GitHub Actions)\nImplement core features in phases\nTest extensively (unit, integration, E2E)\nDeploy to GKE\nIterate based on feedback\n\n\n\n\n\n\n\n\n# tasks/load-market-data.yaml\nname: load-market-data\nglobal: true\nkey: \"load_market_${params.batch_date}_${params.region}\"\n\nparams:\n  batch_date:\n    type: string\n    required: true\n    description: Business date to process (YYYY-MM-DD)\n  \n  region:\n    type: string\n    default: us\n    description: Market region (us, eu, asia)\n\ncommand: dbt\nargs:\n  - run\n  - --models\n  - +market_data\n  - --vars\n  - \"batch_date:${params.batch_date},region:${params.region}\"\n  - --target\n  - prod\n\nenv:\n  DBT_PROFILES_DIR: /dbt/profiles\n  DBT_TARGET: prod\n\ntimeout: 600\nretry: 3\n\n\n\n# graphs/daily-risk-calculation.yaml\nname: daily-risk-calculation\ndescription: |\n  Daily risk calculation pipeline.\n  Runs after market close to calculate VaR and stress scenarios.\n\nparams:\n  batch_date:\n    type: string\n    default: \"${date.today()}\"\n    description: Processing date\n  \n  region:\n    type: string\n    default: us\n    description: Market region\n  \n  confidence_level:\n    type: number\n    default: 0.99\n    description: VaR confidence level\n\nenv:\n  GCS_BUCKET: \"gs://risk-data-${params.region}\"\n  PROCESSING_DATE: \"${params.batch_date}\"\n\nschedule: \"0 18 * * 1-5\"  # 6 PM, weekdays only\n\ntasks:\n  # Global task - shared with other graphs\n  - task: load-market-data\n    params:\n      batch_date: \"${params.batch_date}\"\n      region: \"${params.region}\"\n  \n  # Inline task - specific to this graph\n  - name: calculate-var\n    command: python\n    args:\n      - /opt/risk/var.py\n      - --date\n      - \"${params.batch_date}\"\n      - --region\n      - \"${params.region}\"\n      - --confidence\n      - \"${params.confidence_level}\"\n      - --output\n      - \"${env.GCS_BUCKET}/var/${params.batch_date}.parquet\"\n    env:\n      PYTHONUNBUFFERED: \"1\"\n    timeout: 1800\n    retry: 2\n    depends_on:\n      - load-market-data\n  \n  - name: stress-test\n    command: python\n    args:\n      - /opt/risk/stress.py\n      - --date\n      - \"${params.batch_date}\"\n      - --scenarios\n      - \"recession,spike,crash\"\n      - --input\n      - \"${env.GCS_BUCKET}/var/${params.batch_date}.parquet\"\n    timeout: 3600\n    depends_on:\n      - calculate-var\n  \n  - name: generate-report\n    command: python\n    args:\n      - /opt/risk/report.py\n      - --date\n      - \"${params.batch_date}\"\n      - --var-results\n      - \"${task.calculate-var.result.output_file}\"\n      - --stress-results\n      - \"${task.stress-test.result.output_file}\"\n    depends_on:\n      - calculate-var\n      - stress-test\n\n\n\n# application.yaml\norchestrator:\n  mode: dev\n  \n  config:\n    graphs: ./graphs\n    tasks: ./tasks\n    watch: true\n  \n  dev:\n    worker-threads: 4\n    trial-run: false\n    enable-editor: true\n  \n  storage:\n    events: file://./data/events\n  \n  hazelcast:\n    embedded: true\n    cluster-name: orchestrator-dev\n  \n  worker:\n    threads: 4\n    heartbeat-interval: 10\n    dead-threshold: 60\n\nserver:\n  port: 8080\n\nlogging:\n  level: INFO\n  format: text\n\n---\n\n# application-prod.yaml\norchestrator:\n  mode: prod\n  \n  config:\n    graphs: /config/graphs\n    tasks: /config/tasks\n    watch: false\n  \n  storage:\n    events: gs://my-bucket/orchestrator/events\n  \n  hazelcast:\n    embedded: false\n    cluster-name: orchestrator-prod\n    members:\n      - hazelcast-0.hazelcast.orchestrator.svc.cluster.local\n      - hazelcast-1.hazelcast.orchestrator.svc.cluster.local\n      - hazelcast-2.hazelcast.orchestrator.svc.cluster.local\n  \n  worker:\n    threads: 16\n    heartbeat-interval: 10\n    dead-threshold: 60\n\nserver:\n  port: 8080\n\nlogging:\n  level: INFO\n  format: json\n\nEND OF DESIGN DOCUMENT",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Overview\nArchitecture\nCore Concepts\nConfiguration Schema\nExpression Language\nGlobal Tasks\nParameter Scoping\nDevelopment Mode\nProduction Deployment\nUI Design\nData Model\nImplementation Details\nTesting Strategy\nDeployment Guide",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "A lightweight, portable, event-driven workflow orchestrator that gets out of your way. Designed for data teams who want to ship fast without operational overhead.\nCore Philosophy: - Simple over complex - Portable over locked-in - YAML over code - Files over databases (for events) - Commands over plugins\n\n\n\n\nGlobal Task Deduplication - Run once, notify many graphs\nSingle Binary - No dependencies, runs anywhere\nFile-Based Events - JSONL files, not databases\nMulti-Threaded Workers - Efficient for I/O-bound tasks\nDev Mode - Single process with embedded worker\nTrial Run - See what would execute without executing\n\n\n\n\n\nData engineering teams (5-50 people)\nOrganizations with multi-cloud or hybrid deployments\nTeams that value simplicity and portability\nCost-conscious organizations\nEdge computing scenarios\n\n\n\n\n\nFAANG-scale (millions of tasks/day)\nTeams that need complex multi-tenancy\nMicroservice orchestration (use Temporal)\nTeams deeply invested in Airflow ecosystem",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "┌─────────────────────────────────────────────────────────────┐\n│                    Orchestrator Process                     │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │              Core Orchestration                        │ │\n│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐   │ │\n│  │  │Graph Loader  │  │Event Bus     │  │Graph        │   │ │\n│  │  │(YAML→Memory) │  │(Vert.x)      │  │Evaluator    │   │ │\n│  │  └──────────────┘  └──────────────┘  └─────────────┘   │ │\n│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐   │ │\n│  │  │State Manager │  │Worker Monitor│  │Task Queue   │   │ │\n│  │  │(Hazelcast)   │  │              │  │Publisher    │   │ │\n│  │  └──────────────┘  └──────────────┘  └─────────────┘   │ │\n│  └────────────────────────────────────────────────────────┘ │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │              Worker Pool (Embedded in Dev)             │ │\n│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐    │ │\n│  │  │Thread 1 │  │Thread 2 │  │Thread 3 │  │Thread 4 │    │ │\n│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘    │ │\n│  └────────────────────────────────────────────────────────┘ │\n│                                                             │\n│  ┌────────────────────────────────────────────────────────┐ │\n│  │                    Web UI (SSR)                        │ │\n│  │  ┌──────────────────────────────────────────────────┐  │ │\n│  │  │  Qute Templates + Tabler CSS + Monaco Editor     │  │ │\n│  │  └──────────────────────────────────────────────────┘  │ │\n│  └────────────────────────────────────────────────────────┘ │\n└─────────────────────────────────────────────────────────────┘\n                              │\n                ┌─────────────┴─────────────┐\n                ↓                           ↓\n        ┌──────────────┐          ┌──────────────────┐\n        │ Event Store  │          │  Config Files    │\n        │ (JSONL)      │          │  (YAML)          │\n        │              │          │                  │\n        │ Local: files │          │  graphs/*.yaml   │\n        │ Prod: GCS/S3 │          │  tasks/*.yaml    │\n        └──────────────┘          └──────────────────┘\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│                         GKE Cluster                         │\n│                                                             │\n│  ┌──────────────────┐         ┌─────────────────────────┐   │\n│  │   Orchestrator   │         │   Worker Pods (HPA)     │   │\n│  │    (1 replica)   │         │                         │   │\n│  │                  │         │  ┌────────┐ ┌────────┐  │   │\n│  │  - Graph Eval    │         │  │Worker 1│ │Worker 2│  │   │\n│  │  - State Mgmt    │         │  │16 thrd │ │16 thrd │  │   │\n│  │  - UI (SSR)      │         │  └────────┘ └────────┘  │   │\n│  │  - API           │         │       ...               │   │\n│  └──────────────────┘         └─────────────────────────┘   │\n│         │                                                   │\n└─────────┼───────────────────────────────────────────────────┘\n          │\n    ┌─────┴─────┐\n    ▼           ▼\n┌────────┐  ┌────────────┐      ┌──────────────┐\n│Hazel-  │  │  GCS/S3    │      │ ConfigMap    │\n│cast    │  │            │      │              │\n│Cluster │  │ Events:    │      │ graphs/*.yaml│\n│(3 AZs) │  │ *.jsonl    │      │ tasks/*.yaml │\n└────────┘  └────────────┘      └──────────────┘",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#core-concepts",
    "href": "index.html#core-concepts",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Definition: A single unit of work. Just a command to execute.\nKey Properties: - name - Unique identifier - command - Program to execute - args - Command arguments (with JEXL expressions) - env - Environment variables - timeout - Maximum execution time - retry - Retry policy\nExample:\nname: extract-data\ncommand: python\nargs:\n  - /scripts/extract.py\n  - --date\n  - \"${params.batch_date}\"\nenv:\n  PYTHONUNBUFFERED: \"1\"\n  API_KEY: \"${env.API_KEY}\"\ntimeout: 600  # 10 minutes\nretry: 3\n\n\n\nDefinition: Tasks that can be shared across multiple graphs. Run once per unique parameter set.\nKey Feature: Deduplication via parameterized keys.\nKey Properties: - global: true - Marks as global - key - Expression that uniquely identifies this task instance - params - Parameters with defaults\nExample:\nname: load-market-data\nglobal: true\nkey: \"load_market_${params.batch_date}_${params.region}\"\n\nparams:\n  batch_date:\n    type: string\n    required: true\n  region:\n    type: string\n    default: us\n\ncommand: dbt\nargs:\n  - run\n  - --models\n  - +market_data\n  - --vars\n  - \"batch_date:${params.batch_date},region:${params.region}\"\nBehavior: - Graph A needs load-market-data[2025-10-17, us] - Graph B needs load-market-data[2025-10-17, us] - Task runs once, both graphs notified on completion - Graph C needs load-market-data[2025-10-16, us] - Different key → runs separately\n\n\n\nDefinition: A directed acyclic graph (DAG) of tasks with dependencies.\nKey Properties: - name - Unique identifier - params - Parameters with defaults (can be overridden at runtime) - env - Graph-level environment variables - tasks - List of tasks (inline or references to global tasks) - schedule (optional) - Cron expression for scheduled execution\nExample:\nname: daily-etl\ndescription: Daily ETL pipeline\n\nparams:\n  batch_date:\n    type: string\n    default: \"${date.today()}\"\n  region:\n    type: string\n    default: us\n\nenv:\n  GCS_BUCKET: \"gs://data-${params.region}\"\n  PROCESSING_DATE: \"${params.batch_date}\"\n\nschedule: \"0 2 * * *\"  # 2 AM daily\n\ntasks:\n  # Reference global task\n  - task: load-market-data\n    params:\n      batch_date: \"${params.batch_date}\"\n      region: \"${params.region}\"\n  \n  # Inline task\n  - name: transform-data\n    command: python\n    args:\n      - /scripts/transform.py\n      - --input\n      - \"${env.GCS_BUCKET}/raw/${params.batch_date}\"\n    depends_on:\n      - load-market-data\n\n\n\nDefinition: A single run of a graph with specific parameters.\nLifecycle: 1. RUNNING - Graph is being evaluated/executed 2. COMPLETED - All tasks completed successfully 3. FAILED - One or more tasks failed permanently 4. STALLED - No progress possible (waiting on failed dependencies) 5. PAUSED - Manually paused by user\n\n\n\nDefinition: A single execution of a task within a graph execution.\nLifecycle: 1. PENDING - Task created, waiting for dependencies 2. QUEUED - Published to worker queue 3. RUNNING - Worker is executing 4. COMPLETED - Successfully finished 5. FAILED - Failed (may retry) 6. SKIPPED - Skipped due to upstream failure\n\n\n\nDefinition: Processes that pull tasks from queue and execute them.\nTypes: - Embedded Worker (dev mode) - Runs in same process as orchestrator - Dedicated Workers (prod) - Separate pods with multi-threaded execution\nKey Properties: - worker_id - Unique identifier (hostname/pod name) - threads - Number of concurrent task executions - heartbeat - Periodic health signal (every 10s) - capabilities - What tasks can run (future: heterogeneous workers)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#configuration-schema",
    "href": "index.html#configuration-schema",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Location: tasks/{task-name}.yaml\n# Required: Task name (must match filename without .yaml)\nname: string  # Pattern: ^[a-z0-9-]+$\n\n# Optional: Mark as global task\nglobal: boolean  # Default: false\n\n# Required for global tasks: Unique key expression\nkey: string  # JEXL expression with params\n\n# Optional: Parameters (required for global tasks with key)\nparams:\n  param_name:\n    type: string|integer|boolean|date|array  # Required\n    default: any  # Optional (JEXL expression allowed)\n    required: boolean  # Default: false\n    description: string  # Optional\n\n# Required: Command to execute\ncommand: string\n\n# Required: Command arguments\nargs:\n  - string  # JEXL expressions allowed\n\n# Optional: Environment variables\nenv:\n  KEY: string  # JEXL expressions allowed\n\n# Optional: Timeout in seconds\ntimeout: integer  # Default: 3600 (1 hour)\n\n# Optional: Retry policy\nretry: integer  # Default: 3 (max retry attempts)\n\n\n\nLocation: graphs/{graph-name}.yaml\n# Required: Graph name (must match filename without .yaml)\nname: string  # Pattern: ^[a-z0-9-]+$\n\n# Optional: Human-readable description\ndescription: string\n\n# Optional: Parameters\nparams:\n  param_name:\n    type: string|integer|boolean|date|array\n    default: any  # JEXL expression allowed\n    required: boolean  # Default: false\n    description: string\n\n# Optional: Graph-level environment variables\nenv:\n  KEY: string  # JEXL expressions allowed\n\n# Optional: Schedule (cron expression)\nschedule: string  # e.g., \"0 2 * * *\"\n\n# Optional: Triggers\ntriggers:\n  - type: webhook|pubsub|schedule\n    # Type-specific config\n\n# Required: Tasks\ntasks:\n  # Option 1: Reference global task\n  - task: string  # Global task name\n    params:  # Optional param overrides\n      param_name: any  # JEXL expression\n    depends_on:  # Optional\n      - string  # Task names\n  \n  # Option 2: Inline task definition\n  - name: string\n    command: string\n    args:\n      - string\n    env:\n      KEY: string\n    timeout: integer\n    retry: integer\n    depends_on:\n      - string\n\n\n\nLocation: application.yaml (or application-{profile}.yaml)\norchestrator:\n  # Mode: dev or prod\n  mode: dev\n  \n  # Config paths\n  config:\n    graphs: ./graphs  # Directory containing graph YAML files\n    tasks: ./tasks    # Directory containing task YAML files\n    watch: true       # Watch for file changes (dev only)\n  \n  # Development mode settings\n  dev:\n    worker-threads: 4       # Embedded worker thread count\n    trial-run: false        # If true, only log commands instead of executing\n    enable-editor: true     # Enable web-based YAML editor\n  \n  # Storage\n  storage:\n    events: file://./data/events  # or gs://bucket/events or s3://bucket/events\n  \n  # Hazelcast\n  hazelcast:\n    embedded: true  # true for dev, false for prod\n    cluster-name: orchestrator\n    members:        # Prod only\n      - orchestrator-0.orchestrator\n      - orchestrator-1.orchestrator\n      - orchestrator-2.orchestrator\n  \n  # Worker settings (for dedicated workers)\n  worker:\n    threads: 16             # Threads per worker pod\n    heartbeat-interval: 10  # Seconds\n    dead-threshold: 60      # Seconds\n\n# Web server\nserver:\n  port: 8080\n\n# Logging\nlogging:\n  level: info\n  format: json  # or text",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#expression-language",
    "href": "index.html#expression-language",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "We use Apache Commons JEXL 3 for all expressions. Expressions are wrapped in ${ }.\nKey Features: - Arithmetic: +, -, *, /, % - Comparison: ==, !=, &lt;, &gt;, &lt;=, &gt;= - Logical: &&, ||, ! - Ternary: condition ? true_val : false_val - Null-safe: object?.property - Elvis: value ?: default\n\n\n\n${params.name}          - Access parameter\n${env.NAME}             - Access environment variable\n${task.taskname.result} - Access upstream task result (JSON)\n\n\n\n\n\n${date.today()}                              → \"2025-10-17\"\n${date.now('yyyy-MM-dd HH:mm:ss')}          → \"2025-10-17 14:30:00\"\n${date.add(date.today(), 1, 'days')}        → \"2025-10-18\"\n${date.sub(date.today(), 7, 'days')}        → \"2025-10-10\"\n${date.format(params.date, 'yyyy/MM/dd')}   → \"2025/10/17\"\n\n\n\n${string.uuid()}                             → \"550e8400-e29b-41d4-a716-446655440000\"\n${string.slugify('My Workflow Name')}       → \"my-workflow-name\"\n\n\n\n${'hello'.toUpperCase()}                     → \"HELLO\"\n${'HELLO'.toLowerCase()}                     → \"hello\"\n${'2025-10-17'.replace('-', '_')}           → \"2025_10_17\"\n${'  text  '.trim()}                         → \"text\"\n\n\n\n${Math.round(3.7)}                           → 4\n${Math.floor(3.7)}                           → 3\n${Math.ceil(3.2)}                            → 4\n${Math.max(10, 20)}                          → 20\n${Math.min(10, 20)}                          → 10\n\n\n\n\n# Simple variable substitution\ncommand: echo\nargs:\n  - \"Processing ${params.region}\"\n\n# Arithmetic\nenv:\n  BATCH_SIZE: \"${params.scale * 100}\"\n  WORKER_COUNT: \"${params.scale * 4}\"\n\n# Conditionals\nargs:\n  - \"${params.full_refresh ? '--full-refresh' : '--incremental'}\"\n\n# Complex logic\nenv:\n  PARALLELISM: \"${params.scale &gt; 10 ? 32 : params.scale &gt; 5 ? 16 : 4}\"\n\n# String manipulation\nenv:\n  TABLE_NAME: \"${'data_' + params.region + '_' + params.date.replace('-', '_')}\"\n\n# Null-safe access\nenv:\n  API_KEY: \"${params.config?.api?.key ?: env.DEFAULT_API_KEY}\"\n\n# Date operations\nparams:\n  yesterday:\n    default: \"${date.add(date.today(), -1, 'days')}\"\n\n# Reference upstream task output\nargs:\n  - --row-count\n  - \"${task.extract.result.row_count}\"",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#global-tasks-1",
    "href": "index.html#global-tasks-1",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Global tasks solve the problem of multiple graphs needing the same data/computation for the same parameters.\nWithout global tasks:\nGraph A: load-data[2025-10-17] → runs\nGraph B: load-data[2025-10-17] → runs (duplicate!)\nGraph C: load-data[2025-10-17] → runs (duplicate!)\nWith global tasks:\nGraph A: load-data[2025-10-17] → starts execution\nGraph B: load-data[2025-10-17] → links to same execution\nGraph C: load-data[2025-10-17] → links to same execution\n→ Runs once, all three graphs proceed together\n\n\n\nThe key field uniquely identifies a task instance. It should include all parameters that affect the task’s output.\nGood Keys:\n# Includes date and region - different dates/regions = different data\nkey: \"load_data_${params.batch_date}_${params.region}\"\n\n# Includes all meaningful params\nkey: \"bootstrap_curves_${params.date}_${params.region}_${params.model_version}\"\nBad Keys:\n# Too generic - all graphs share same execution even with different params\nkey: \"load_data\"\n\n# Includes volatile data - every execution is \"unique\"\nkey: \"load_data_${params.batch_date}_${date.now()}\"\n\n\n\nGlobal tasks are tracked in Hazelcast:\n// Key structure\nrecord TaskExecutionKey(\n    String taskName,\n    String resolvedKey  // Evaluated expression\n) {}\n\n// State\nrecord GlobalTaskExecution(\n    UUID id,\n    String taskName,\n    String resolvedKey,\n    Map&lt;String, Object&gt; params,\n    TaskStatus status,\n    Set&lt;UUID&gt; linkedGraphExecutions,  // Which graphs are waiting\n    Instant startedAt,\n    Instant completedAt,\n    Map&lt;String, Object&gt; result\n) {}\n\n\n\n\nGraph A evaluates, needs global task with key load_data_2025-10-17_us\nCheck Hazelcast for existing execution with that key\nNot found → Start new execution, add Graph A to linked graphs\nGraph B evaluates, needs same key\nFound, status=RUNNING → Add Graph B to linked graphs, don’t start new execution\nTask completes → Notify both Graph A and Graph B\nBoth graphs re-evaluate and schedule downstream tasks",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#parameter-scoping",
    "href": "index.html#parameter-scoping",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Runtime Invocation - Params passed when triggering graph\nGraph Task Reference - Params in graph’s task definition\nGlobal Task Defaults - Defaults in global task definition\nGraph Defaults - Defaults in graph params\n\n\n\n\nGlobal Task:\n# tasks/process-data.yaml\nname: process-data\nglobal: true\nkey: \"process_${params.date}_${params.region}\"\n\nparams:\n  date:\n    type: string\n    required: true\n  region:\n    type: string\n    default: us      # Level 3: Global task default\n  threads:\n    type: integer\n    default: 4       # Level 3: Global task default\nGraph:\n# graphs/etl-pipeline.yaml\nname: etl-pipeline\n\nparams:\n  date:\n    type: string\n    default: \"${date.today()}\"  # Level 4: Graph default\n  region:\n    type: string\n    default: eu                 # Level 4: Graph default (overrides global)\n\ntasks:\n  - task: process-data\n    params:\n      date: \"${params.date}\"\n      region: \"${params.region}\"\n      threads: 8                # Level 2: Task reference override\nRuntime Invocation:\ncurl -X POST /api/graphs/etl-pipeline/execute \\\n  -d '{\"params\": {\"date\": \"2025-10-15\", \"region\": \"asia\"}}'\n# Level 1: Runtime override (highest priority)\nFinal Resolution:\ndate: \"2025-10-15\"    (Level 1: runtime)\nregion: \"asia\"        (Level 1: runtime)\nthreads: 8            (Level 2: task reference)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#development-mode",
    "href": "index.html#development-mode",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "In dev mode, everything runs in one process:\n┌─────────────────────────────────────────┐\n│      Single JVM Process                 │\n│                                         │\n│  Orchestrator Core                      │\n│  ↓                                      │\n│  Embedded Hazelcast                     │\n│  ↓                                      │\n│  Embedded Worker Pool (4 threads)       │\n│  ↓                                      │\n│  Local Event Store (./data/events)      │\n│  ↓                                      │\n│  Web UI (http://localhost:8080)         │\n└─────────────────────────────────────────┘\nStart command:\n./orchestrator --profile=dev\n# or\njava -jar orchestrator.jar --profile=dev\nOutput:\n[INFO] Orchestrator starting in DEV mode\n[INFO] Loading graphs from ./graphs\n[INFO]   - daily-etl.yaml\n[INFO]   - market-pipeline.yaml\n[INFO] Loading tasks from ./tasks\n[INFO]   - load-market-data.yaml\n[INFO]   - bootstrap-curves.yaml\n[INFO] Starting embedded Hazelcast cluster\n[INFO] Hazelcast cluster formed (1 member)\n[INFO] Starting embedded worker pool with 4 threads\n[INFO]   - worker-thread-1 ready\n[INFO]   - worker-thread-2 ready\n[INFO]   - worker-thread-3 ready\n[INFO]   - worker-thread-4 ready\n[INFO] Starting web server on port 8080\n[INFO] \n[INFO] ============================================\n[INFO] Orchestrator ready!\n[INFO] UI: http://localhost:8080\n[INFO] Mode: DEVELOPMENT\n[INFO] Trial Run: DISABLED\n[INFO] ============================================\n\n\n\nIn dev mode, watch for file changes:\n@ApplicationScoped\n@RequiresProfile(\"dev\")\npublic class ConfigWatcher {\n    @ConfigProperty(name = \"orchestrator.config.watch\")\n    boolean watchEnabled;\n    \n    void onStart(@Observes StartupEvent event) {\n        if (watchEnabled) {\n            startWatching();\n        }\n    }\n    \n    private void startWatching() {\n        WatchService watchService = FileSystems.getDefault().newWatchService();\n        \n        Path graphsDir = Path.of(\"./graphs\");\n        Path tasksDir = Path.of(\"./tasks\");\n        \n        graphsDir.register(watchService, ENTRY_MODIFY, ENTRY_CREATE);\n        tasksDir.register(watchService, ENTRY_MODIFY, ENTRY_CREATE);\n        \n        new Thread(() -&gt; {\n            while (true) {\n                WatchKey key = watchService.take();\n                \n                for (WatchEvent&lt;?&gt; event : key.pollEvents()) {\n                    Path changed = (Path) event.context();\n                    logger.info(\"Config file changed: {}\", changed);\n                    \n                    // Reload\n                    if (changed.toString().endsWith(\".yaml\")) {\n                        reloadConfig(changed);\n                    }\n                }\n                \n                key.reset();\n            }\n        }).start();\n    }\n    \n    private void reloadConfig(Path file) {\n        try {\n            if (file.getParent().endsWith(\"graphs\")) {\n                graphLoader.reloadGraph(file);\n            } else if (file.getParent().endsWith(\"tasks\")) {\n                graphLoader.reloadTask(file);\n            }\n            logger.info(\"✓ Config reloaded: {}\", file);\n        } catch (Exception e) {\n            logger.error(\"✗ Failed to reload config: {}\", file, e);\n        }\n    }\n}\n\n\n\nExecute graphs without actually running commands. Perfect for testing configuration.\nEnable:\n# application-dev.yaml\norchestrator:\n  dev:\n    trial-run: true\nBehavior:\n@ApplicationScoped\npublic class TaskExecutor {\n    @ConfigProperty(name = \"orchestrator.dev.trial-run\")\n    boolean trialRun;\n    \n    public TaskResult execute(WorkMessage work) {\n        String fullCommand = buildCommandString(work);\n        \n        if (trialRun) {\n            // Log what would execute, but don't execute\n            logger.info(\"TRIAL RUN - Would execute: {}\", fullCommand);\n            logger.info(\"  Working dir: {}\", work.workingDir());\n            logger.info(\"  Environment: {}\", work.env());\n            logger.info(\"  Timeout: {}s\", work.timeoutSeconds());\n            \n            // Return fake success\n            return TaskResult.success(Map.of(\n                \"trial_run\", true,\n                \"command\", fullCommand\n            ));\n        }\n        \n        // Real execution\n        return executeCommand(work);\n    }\n}\nOutput:\n[INFO] Graph execution started: daily-etl (trial-run)\n[INFO] Task: load-market-data\n[INFO]   TRIAL RUN - Would execute: dbt run --models +market_data --vars batch_date:2025-10-17\n[INFO]   Working dir: /opt/dbt\n[INFO]   Environment: {DBT_PROFILES_DIR=/dbt/profiles, DBT_TARGET=dev}\n[INFO]   Timeout: 600s\n[INFO] Task: transform-data\n[INFO]   TRIAL RUN - Would execute: python /scripts/transform.py --date 2025-10-17\n[INFO]   Working dir: /opt/scripts\n[INFO]   Environment: {PYTHONUNBUFFERED=1}\n[INFO]   Timeout: 300s\n[INFO] Graph execution completed: daily-etl (trial-run) - 0.5s\n\n\n\nIn dev mode, enable in-browser editing of graphs and tasks.\nEnable:\norchestrator:\n  dev:\n    enable-editor: true\nUI Route: /editor\nFeatures: - Monaco Editor (VS Code’s editor) - YAML syntax highlighting - Real-time validation - Save to file - Auto-reload on save\nImplementation:\n&lt;!-- editor.html --&gt;\n&lt;div class=\"page-body\"&gt;\n  &lt;div class=\"container-xl\"&gt;\n    &lt;div class=\"row\"&gt;\n      &lt;div class=\"col-3\"&gt;\n        &lt;div class=\"card\"&gt;\n          &lt;div class=\"card-header\"&gt;\n            &lt;h3 class=\"card-title\"&gt;Files&lt;/h3&gt;\n          &lt;/div&gt;\n          &lt;div class=\"list-group list-group-flush\"&gt;\n            &lt;div class=\"list-group-item\"&gt;\n              &lt;div class=\"text-muted\"&gt;Graphs&lt;/div&gt;\n              {#for graph in graphs}\n              &lt;a href=\"#\" onclick=\"loadFile('graphs/{graph}.yaml')\" \n                 class=\"list-group-item list-group-item-action\"&gt;\n                {graph}.yaml\n              &lt;/a&gt;\n              {/for}\n            &lt;/div&gt;\n            &lt;div class=\"list-group-item\"&gt;\n              &lt;div class=\"text-muted\"&gt;Tasks&lt;/div&gt;\n              {#for task in tasks}\n              &lt;a href=\"#\" onclick=\"loadFile('tasks/{task}.yaml')\" \n                 class=\"list-group-item list-group-item-action\"&gt;\n                {task}.yaml\n              &lt;/a&gt;\n              {/for}\n            &lt;/div&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n      \n      &lt;div class=\"col-9\"&gt;\n        &lt;div class=\"card\"&gt;\n          &lt;div class=\"card-header\"&gt;\n            &lt;h3 class=\"card-title\" id=\"filename\"&gt;&lt;/h3&gt;\n            &lt;div class=\"card-actions\"&gt;\n              &lt;button class=\"btn btn-primary\" onclick=\"saveFile()\"&gt;\n                &lt;i class=\"ti ti-device-floppy\"&gt;&lt;/i&gt; Save\n              &lt;/button&gt;\n              &lt;button class=\"btn btn-secondary\" onclick=\"validateFile()\"&gt;\n                &lt;i class=\"ti ti-check\"&gt;&lt;/i&gt; Validate\n              &lt;/button&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n          &lt;div class=\"card-body p-0\"&gt;\n            &lt;div id=\"editor\" style=\"height: 600px;\"&gt;&lt;/div&gt;\n          &lt;/div&gt;\n          &lt;div class=\"card-footer\" id=\"validation-result\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;script src=\"https://cdn.jsdelivr.net/npm/monaco-editor@latest/min/vs/loader.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\nlet editor;\nlet currentFile;\n\nrequire.config({ \n  paths: { \n    vs: 'https://cdn.jsdelivr.net/npm/monaco-editor@latest/min/vs' \n  }\n});\n\nrequire(['vs/editor/editor.main'], function() {\n  editor = monaco.editor.create(document.getElementById('editor'), {\n    language: 'yaml',\n    theme: 'vs-dark',\n    automaticLayout: true,\n    minimap: { enabled: false }\n  });\n});\n\nasync function loadFile(path) {\n  currentFile = path;\n  document.getElementById('filename').textContent = path;\n  \n  const response = await fetch('/api/editor/load?path=' + path);\n  const content = await response.text();\n  \n  editor.setValue(content);\n}\n\nasync function saveFile() {\n  const content = editor.getValue();\n  \n  const response = await fetch('/api/editor/save', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      path: currentFile,\n      content: content\n    })\n  });\n  \n  if (response.ok) {\n    showValidation('✓ File saved successfully', 'success');\n  } else {\n    const error = await response.text();\n    showValidation('✗ Save failed: ' + error, 'danger');\n  }\n}\n\nasync function validateFile() {\n  const content = editor.getValue();\n  \n  const response = await fetch('/api/editor/validate', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      path: currentFile,\n      content: content\n    })\n  });\n  \n  const result = await response.json();\n  \n  if (result.valid) {\n    showValidation('✓ Valid YAML', 'success');\n  } else {\n    showValidation('✗ ' + result.error, 'danger');\n  }\n}\n\nfunction showValidation(message, type) {\n  const resultDiv = document.getElementById('validation-result');\n  resultDiv.innerHTML = `&lt;div class=\"alert alert-${type}\"&gt;${message}&lt;/div&gt;`;\n  setTimeout(() =&gt; {\n    resultDiv.innerHTML = '';\n  }, 3000);\n}\n&lt;/script&gt;\nAPI Endpoints:\n@Path(\"/api/editor\")\n@RequiresProfile(\"dev\")\npublic class EditorController {\n    @ConfigProperty(name = \"orchestrator.config.graphs\")\n    String graphsPath;\n    \n    @ConfigProperty(name = \"orchestrator.config.tasks\")\n    String tasksPath;\n    \n    @GET\n    @Path(\"/load\")\n    public String loadFile(@QueryParam(\"path\") String path) {\n        Path filePath = resolveSecurePath(path);\n        return Files.readString(filePath);\n    }\n    \n    @POST\n    @Path(\"/save\")\n    public Response saveFile(SaveRequest request) {\n        Path filePath = resolveSecurePath(request.path());\n        \n        // Validate first\n        ValidationResult validation = validator.validate(request.content());\n        if (!validation.isValid()) {\n            return Response.status(400).entity(validation.error()).build();\n        }\n        \n        // Write to file\n        Files.writeString(filePath, request.content());\n        \n        // Trigger reload\n        if (request.path().startsWith(\"graphs/\")) {\n            graphLoader.reloadGraph(filePath);\n        } else {\n            taskLoader.reloadTask(filePath);\n        }\n        \n        return Response.ok().build();\n    }\n    \n    @POST\n    @Path(\"/validate\")\n    public ValidationResult validateFile(ValidateRequest request) {\n        return validator.validate(request.content());\n    }\n    \n    private Path resolveSecurePath(String path) {\n        // Prevent directory traversal\n        Path base = Path.of(graphsPath).getParent();\n        Path resolved = base.resolve(path).normalize();\n        \n        if (!resolved.startsWith(base)) {\n            throw new SecurityException(\"Invalid path\");\n        }\n        \n        return resolved;\n    }\n}",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#production-deployment",
    "href": "index.html#production-deployment",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Components: 1. Orchestrator (1 replica) - Stateless, can be restarted 2. Worker Pods (2-20 replicas, HPA) - Pull work from Hazelcast queue 3. Hazelcast Cluster (3 nodes, 1 per AZ) - State storage 4. ConfigMap - Graph and task YAML files 5. GCS - Event log storage\n\n\n\nStatefulSet:\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: hazelcast\n  namespace: orchestrator\nspec:\n  serviceName: hazelcast\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hazelcast\n  template:\n    metadata:\n      labels:\n        app: hazelcast\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - hazelcast\n            topologyKey: topology.kubernetes.io/zone\n      containers:\n      - name: hazelcast\n        image: hazelcast/hazelcast:5.3\n        env:\n        - name: JAVA_OPTS\n          value: \"-Xms2g -Xmx2g\"\n        ports:\n        - containerPort: 5701\n        resources:\n          requests:\n            memory: 2Gi\n            cpu: 500m\n          limits:\n            memory: 4Gi\n            cpu: 2000m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hazelcast\n  namespace: orchestrator\nspec:\n  clusterIP: None\n  selector:\n    app: hazelcast\n  ports:\n  - port: 5701\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orchestrator\n  namespace: orchestrator\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate  # Kill old before starting new\n  selector:\n    matchLabels:\n      app: orchestrator\n  template:\n    metadata:\n      labels:\n        app: orchestrator\n    spec:\n      serviceAccountName: orchestrator-sa\n      containers:\n      - name: orchestrator\n        image: gcr.io/project/orchestrator:latest\n        env:\n        - name: QUARKUS_PROFILE\n          value: prod\n        - name: ORCHESTRATOR_MODE\n          value: prod\n        - name: HAZELCAST_MEMBERS\n          value: \"hazelcast-0.hazelcast,hazelcast-1.hazelcast,hazelcast-2.hazelcast\"\n        - name: GCS_EVENTS_PATH\n          value: \"gs://my-bucket/orchestrator/events\"\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 1Gi\n            cpu: 500m\n          limits:\n            memory: 2Gi\n            cpu: 1000m\n        livenessProbe:\n          httpGet:\n            path: /q/health/live\n            port: 8080\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n          initialDelaySeconds: 10\n        volumeMounts:\n        - name: config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: workflow-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: orchestrator\n  namespace: orchestrator\nspec:\n  type: LoadBalancer\n  selector:\n    app: orchestrator\n  ports:\n  - port: 80\n    targetPort: 8080\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workers\n  namespace: orchestrator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: worker\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      serviceAccountName: worker-sa\n      containers:\n      - name: worker\n        image: gcr.io/project/orchestrator-worker:latest\n        env:\n        - name: QUARKUS_PROFILE\n          value: prod\n        - name: WORKER_THREADS\n          value: \"16\"\n        - name: WORKER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: HAZELCAST_MEMBERS\n          value: \"hazelcast-0.hazelcast,hazelcast-1.hazelcast,hazelcast-2.hazelcast\"\n        resources:\n          requests:\n            memory: 2Gi\n            cpu: 1000m\n          limits:\n            memory: 4Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: config\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: workflow-config\n      terminationGracePeriodSeconds: 300  # Let tasks finish\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: workers-hpa\n  namespace: orchestrator\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: workers\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n      - type: Pods\n        value: 4\n        periodSeconds: 15\n\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workflow-config\n  namespace: orchestrator\ndata:\n  # Tasks\n  load-market-data.yaml: |\n    name: load-market-data\n    global: true\n    key: \"load_market_${params.batch_date}_${params.region}\"\n    params:\n      batch_date:\n        type: string\n        required: true\n      region:\n        type: string\n        default: us\n    command: dbt\n    args:\n      - run\n      - --models\n      - +market_data\n    timeout: 600\n  \n  # Graphs\n  daily-etl.yaml: |\n    name: daily-etl\n    params:\n      batch_date:\n        type: string\n        default: \"${date.today()}\"\n    schedule: \"0 2 * * *\"\n    tasks:\n      - task: load-market-data\n        params:\n          batch_date: \"${params.batch_date}\"\n\n\n\nProcess: 1. Edit YAML files locally or in Git 2. Update ConfigMap 3. Restart orchestrator pod to reload config\n# Update ConfigMap from directory\nkubectl create configmap workflow-config \\\n  --from-file=graphs/ \\\n  --from-file=tasks/ \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n# Restart orchestrator to reload\nkubectl rollout restart deployment/orchestrator -n orchestrator\n\n# Wait for restart\nkubectl rollout status deployment/orchestrator -n orchestrator\nCI/CD Integration:\n# .github/workflows/deploy-workflows.yml\nname: Deploy Workflows\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'graphs/**'\n      - 'tasks/**'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Authenticate to GCP\n        uses: google-github-actions/auth@v1\n        with:\n          credentials_json: ${{ secrets.GCP_SA_KEY }}\n      \n      - name: Setup gcloud\n        uses: google-github-actions/setup-gcloud@v1\n      \n      - name: Get GKE credentials\n        run: |\n          gcloud container clusters get-credentials orchestrator \\\n            --region us-central1 --project my-project\n      \n      - name: Update ConfigMap\n        run: |\n          kubectl create configmap workflow-config \\\n            --from-file=graphs/ \\\n            --from-file=tasks/ \\\n            --dry-run=client -o yaml | kubectl apply -f -\n      \n      - name: Restart Orchestrator\n        run: |\n          kubectl rollout restart deployment/orchestrator -n orchestrator\n          kubectl rollout status deployment/orchestrator -n orchestrator",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#ui-design",
    "href": "index.html#ui-design",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Framework: Qute (Quarkus templating)\nCSS: Tabler.io (MIT license, comprehensive UI kit)\nIcons: Tabler Icons\nCharts: Chart.js\nDAG Visualization: dagre-d3\nGantt Charts: vis-timeline\nEditor: Monaco Editor (VS Code)\nInteractivity: Vanilla JavaScript + Server-Sent Events (SSE)\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│ Orchestrator                              [User] [Settings] │\n├─────┬───────────────────────────────────────────────────────┤\n│     │                                                       │\n│ Nav │  Page Content                                         │\n│     │                                                       │\n│     │                                                       │\n│Dash │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Grph │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Task │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Wrkr │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Evnt │                                                       │\n│     │                                                       │\n│     │                                                       │\n│Sett │                                                       │\n│     │                                                       │\n└─────┴───────────────────────────────────────────────────────┘\n\n\n\n\n\nPurpose: Overview of system health and recent activity\nContent: - Stats cards: Active graphs, queued tasks, active workers, success rate - Recent executions table - Active graphs list with status - System health indicators\n\n\n\nPurpose: Browse all available graphs\nContent: - Searchable/filterable table - Columns: Name, Description, Last Run, Status, Actions - Actions: Execute, View, History\n\n\n\nPurpose: View graph execution in real-time\nTabs: 1. Topology - DAG visualization with color-coded nodes 2. Tasks - Table of all tasks with status, duration, logs link 3. Gantt - Timeline view of execution 4. Logs - Aggregated logs from all tasks\nActions: - Execute (with param form) - Pause/Resume - View History\n\n\n\nPurpose: View past executions\nContent: - Table of executions with filters (date range, status) - Click to view specific execution\n\n\n\nPurpose: View all global tasks and their current state\nContent: - Table of global tasks - Active executions (which graphs are using them) - Recent completions\n\n\n\nPurpose: Monitor worker health and utilization\nContent: - Table of workers with status, threads, active tasks - Worker metrics (CPU, memory if available) - Heartbeat indicators\n\n\n\nPurpose: View event log\nContent: - Searchable/filterable event stream - Real-time updates - Download events as JSONL\n\n\n\nPurpose: Edit graphs and tasks in browser\nContent: - File tree (graphs, tasks) - Monaco editor with YAML syntax highlighting - Save to file - Validation\n\n\n\nPurpose: System configuration\nContent: - Nuclear options (restart, hard reset) - System info - Configuration display\n\n\n\n\nUse Server-Sent Events (SSE) for live updates:\n@Path(\"/api/stream\")\npublic class StreamController {\n    @Inject\n    EventBus eventBus;\n    \n    @GET\n    @Path(\"/graphs/{id}\")\n    @Produces(MediaType.SERVER_SENT_EVENTS)\n    public Multi&lt;GraphEvent&gt; streamGraphEvents(@PathParam(\"id\") UUID graphId) {\n        return Multi.createFrom().emitter(emitter -&gt; {\n            Consumer&lt;Message&lt;Object&gt;&gt; handler = msg -&gt; {\n                GraphEvent event = convertToGraphEvent(msg.body());\n                emitter.emit(event);\n            };\n            \n            MessageConsumer&lt;Object&gt; consumer = eventBus.consumer(\"graph.\" + graphId);\n            consumer.handler(handler);\n            \n            emitter.onTermination(() -&gt; consumer.unregister());\n        });\n    }\n}\n// Client-side\nconst eventSource = new EventSource('/api/stream/graphs/123');\n\neventSource.addEventListener('task-status', (event) =&gt; {\n  const data = JSON.parse(event.data);\n  updateTaskRow(data.taskId, data.status);\n});\n\neventSource.addEventListener('graph-status', (event) =&gt; {\n  const data = JSON.parse(event.data);\n  updateGraphStatus(data.status);\n});",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-model",
    "href": "index.html#data-model",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "// Graph definitions (loaded from YAML)\nIMap&lt;String, Graph&gt; graphDefinitions;\n\n// Task definitions (loaded from YAML)\nIMap&lt;String, Task&gt; taskDefinitions;\n\n// Graph executions (current state)\nIMap&lt;UUID, GraphExecution&gt; graphExecutions;\n\n// Task executions (current state)\nIMap&lt;UUID, TaskExecution&gt; taskExecutions;\n\n// Global task executions (deduplicated)\nIMap&lt;TaskExecutionKey, GlobalTaskExecution&gt; globalTasks;\n\n// Worker registry\nIMap&lt;String, Worker&gt; workers;\n\n\n\n// Work queue (tasks waiting for workers)\nIQueue&lt;WorkMessage&gt; workQueue;\n\n\n\n\nFile Structure:\nevents/\n├── 2025-10-15.jsonl\n├── 2025-10-16.jsonl\n└── 2025-10-17.jsonl\nEvent Types:\n{\"event_type\":\"GRAPH_STARTED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"graph_execution_id\":\"uuid\",\"graph_name\":\"daily-etl\",\"triggered_by\":\"schedule\",\"params\":{\"batch_date\":\"2025-10-17\"}}\n\n{\"event_type\":\"TASK_QUEUED\",\"timestamp\":\"2025-10-17T10:00:01Z\",\"task_execution_id\":\"uuid\",\"task_name\":\"load-data\",\"graph_execution_id\":\"uuid\"}\n\n{\"event_type\":\"TASK_STARTED\",\"timestamp\":\"2025-10-17T10:00:05Z\",\"task_execution_id\":\"uuid\",\"worker_id\":\"worker-1\",\"thread\":\"worker-1-thread-3\"}\n\n{\"event_type\":\"TASK_COMPLETED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"task_execution_id\":\"uuid\",\"duration_seconds\":295,\"result\":{\"rows_loaded\":150000}}\n\n{\"event_type\":\"TASK_FAILED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"task_execution_id\":\"uuid\",\"error\":\"Connection timeout\",\"attempt\":1}\n\n{\"event_type\":\"GRAPH_COMPLETED\",\"timestamp\":\"2025-10-17T10:30:00Z\",\"graph_execution_id\":\"uuid\",\"status\":\"COMPLETED\",\"duration_seconds\":1800}\n\n{\"event_type\":\"GLOBAL_TASK_STARTED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"task_name\":\"load-market-data\",\"resolved_key\":\"load_market_2025-10-17_us\",\"params\":{\"batch_date\":\"2025-10-17\",\"region\":\"us\"},\"initiated_by_graph\":\"uuid\"}\n\n{\"event_type\":\"GLOBAL_TASK_LINKED\",\"timestamp\":\"2025-10-17T10:05:00Z\",\"resolved_key\":\"load_market_2025-10-17_us\",\"linked_graph\":\"uuid\"}\n\n{\"event_type\":\"GLOBAL_TASK_COMPLETED\",\"timestamp\":\"2025-10-17T10:15:00Z\",\"resolved_key\":\"load_market_2025-10-17_us\",\"result\":{\"rows\":1500000}}\n\n{\"event_type\":\"WORKER_HEARTBEAT\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"worker_id\":\"worker-1\",\"active_threads\":3,\"total_threads\":16}\n\n{\"event_type\":\"WORKER_DIED\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"worker_id\":\"worker-2\"}\n\n{\"event_type\":\"CLUSTER_RESTART\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"initiated_by\":\"admin@company.com\"}\n\n{\"event_type\":\"CLUSTER_RESET\",\"timestamp\":\"2025-10-17T10:00:00Z\",\"initiated_by\":\"admin@company.com\"}\n\n\n\n@ApplicationScoped\npublic class StateRecoveryService {\n    void onStart(@Observes StartupEvent event) {\n        // Find last checkpoint or reset event\n        Instant recoverySince = findRecoveryPoint();\n        \n        // Read events since recovery point\n        Stream&lt;Event&gt; events = eventStore.readSince(recoverySince);\n        \n        // Replay events to rebuild state\n        events.forEach(this::replayEvent);\n        \n        // Clean up stale state\n        cleanupStaleExecutions();\n    }\n    \n    private Instant findRecoveryPoint() {\n        // Look for CLUSTER_RESET events\n        Optional&lt;Event&gt; reset = eventStore.findLastEvent(\"CLUSTER_RESET\");\n        if (reset.isPresent()) {\n            return reset.get().timestamp();\n        }\n        \n        // Default: recover last 24 hours\n        return Instant.now().minus(24, ChronoUnit.HOURS);\n    }\n}",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#implementation-details",
    "href": "index.html#implementation-details",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "@ApplicationScoped\npublic class GraphLoader {\n    @Inject\n    ExpressionEvaluator expressionEvaluator;\n    @Inject\n    GraphValidator graphValidator;\n    @Inject\n    HazelcastInstance hazelcast;\n    \n    @ConfigProperty(name = \"orchestrator.config.graphs\")\n    String graphsPath;\n    \n    @ConfigProperty(name = \"orchestrator.config.tasks\")\n    String tasksPath;\n    \n    private IMap&lt;String, Graph&gt; graphDefinitions;\n    private IMap&lt;String, Task&gt; taskDefinitions;\n    \n    void onStart(@Observes StartupEvent event) {\n        graphDefinitions = hazelcast.getMap(\"graph-definitions\");\n        taskDefinitions = hazelcast.getMap(\"task-definitions\");\n        \n        loadTasks();\n        loadGraphs();\n    }\n    \n    private void loadTasks() {\n        try (Stream&lt;Path&gt; paths = Files.walk(Path.of(tasksPath))) {\n            paths.filter(p -&gt; p.toString().endsWith(\".yaml\"))\n                 .forEach(this::loadTask);\n        }\n    }\n    \n    private void loadTask(Path path) {\n        String yaml = Files.readString(path);\n        Task task = parseTask(yaml);\n        \n        // Validate\n        validator.validateTask(task);\n        \n        // Store\n        taskDefinitions.put(task.name(), task);\n        \n        logger.info(\"Loaded task: {}\", task.name());\n    }\n    \n    private void loadGraphs() {\n        try (Stream&lt;Path&gt; paths = Files.walk(Path.of(graphsPath))) {\n            paths.filter(p -&gt; p.toString().endsWith(\".yaml\"))\n                 .forEach(this::loadGraph);\n        }\n    }\n    \n    private void loadGraph(Path path) {\n        String yaml = Files.readString(path);\n        Graph graph = parseGraph(yaml);\n        \n        // Validate\n        graphValidator.validateGraph(graph);\n        \n        // Store\n        graphDefinitions.put(graph.name(), graph);\n        \n        logger.info(\"Loaded graph: {} ({} tasks)\", \n            graph.name(), graph.tasks().size());\n    }\n}\n\n\n\n@ApplicationScoped\npublic class GraphEvaluator {\n    @Inject\n    EventBus eventBus;\n    @Inject\n    JGraphTService jGraphTService;\n    @Inject\n    HazelcastInstance hazelcast;\n    \n    private IMap&lt;UUID, GraphExecution&gt; graphExecutions;\n    private IMap&lt;UUID, TaskExecution&gt; taskExecutions;\n    private IMap&lt;TaskExecutionKey, GlobalTaskExecution&gt; globalTasks;\n    \n    @PostConstruct\n    void init() {\n        graphExecutions = hazelcast.getMap(\"graph-executions\");\n        taskExecutions = hazelcast.getMap(\"task-executions\");\n        globalTasks = hazelcast.getMap(\"global-tasks\");\n    }\n    \n    @ConsumeEvent(\"task.completed\")\n    @ConsumeEvent(\"task.failed\")\n    public void onTaskStatusChange(TaskEvent event) {\n        // Update task execution\n        TaskExecution te = taskExecutions.get(event.executionId());\n        te = te.withStatus(event.status());\n        taskExecutions.put(event.executionId(), te);\n        \n        // Find graph execution(s) to re-evaluate\n        if (te.isGlobal()) {\n            // Global task - notify all linked graphs\n            GlobalTaskExecution gte = globalTasks.get(te.globalKey());\n            gte.linkedGraphExecutions().forEach(graphExecId -&gt; \n                eventBus.publish(\"graph.evaluate\", graphExecId)\n            );\n        } else {\n            // Regular task - notify its graph\n            eventBus.publish(\"graph.evaluate\", te.graphExecutionId());\n        }\n    }\n    \n    @ConsumeEvent(\"graph.evaluate\")\n    public void evaluate(UUID graphExecutionId) {\n        GraphExecution exec = graphExecutions.get(graphExecutionId);\n        \n        // Build DAG\n        DirectedAcyclicGraph&lt;TaskNode, DefaultEdge&gt; dag = \n            jGraphTService.buildDAG(exec.graph());\n        \n        // Get current state\n        Map&lt;TaskNode, TaskStatus&gt; state = getCurrentState(graphExecutionId);\n        \n        // Find ready tasks\n        Set&lt;TaskNode&gt; ready = jGraphTService.findReadyTasks(dag, state);\n        \n        // Schedule each ready task\n        ready.forEach(task -&gt; scheduleTask(graphExecutionId, task));\n        \n        // Update graph status\n        updateGraphStatus(graphExecutionId, state);\n    }\n    \n    private void scheduleTask(UUID graphExecutionId, TaskNode node) {\n        if (node.isGlobal()) {\n            scheduleGlobalTask(graphExecutionId, node);\n        } else {\n            scheduleRegularTask(graphExecutionId, node);\n        }\n    }\n}\n\n\n\n@ApplicationScoped\npublic class WorkerPool {\n    @Inject\n    HazelcastInstance hazelcast;\n    @Inject\n    TaskExecutor taskExecutor;\n    @Inject\n    EventLogger eventLogger;\n    \n    @ConfigProperty(name = \"worker.threads\")\n    int workerThreads;\n    \n    @ConfigProperty(name = \"worker.id\")\n    String workerId;\n    \n    private IQueue&lt;WorkMessage&gt; workQueue;\n    private ExecutorService executorService;\n    private volatile boolean running = false;\n    \n    @PostConstruct\n    void init() {\n        workQueue = hazelcast.getQueue(\"work-queue\");\n    }\n    \n    public void start(int threads) {\n        this.workerThreads = threads;\n        this.running = true;\n        \n        // Create thread pool\n        this.executorService = Executors.newFixedThreadPool(\n            workerThreads,\n            new ThreadFactory() {\n                private final AtomicInteger counter = new AtomicInteger(0);\n                \n                @Override\n                public Thread newThread(Runnable r) {\n                    Thread t = new Thread(r);\n                    t.setName(workerId + \"-thread-\" + counter.incrementAndGet());\n                    return t;\n                }\n            }\n        );\n        \n        // Start worker threads\n        for (int i = 0; i &lt; workerThreads; i++) {\n            executorService.submit(this::workerLoop);\n        }\n        \n        // Start heartbeat\n        startHeartbeat();\n        \n        logger.info(\"Worker pool started: {} threads\", workerThreads);\n    }\n    \n    private void workerLoop() {\n        String threadName = Thread.currentThread().getName();\n        \n        while (running) {\n            try {\n                // Pull work (blocking with timeout)\n                WorkMessage work = workQueue.poll(5, TimeUnit.SECONDS);\n                \n                if (work == null) {\n                    continue;  // Timeout, try again\n                }\n                \n                executeWork(work, threadName);\n                \n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n                break;\n            } catch (Exception e) {\n                logger.error(\"[{}] Error in worker loop\", threadName, e);\n            }\n        }\n    }\n    \n    private void executeWork(WorkMessage work, String threadName) {\n        logger.info(\"[{}] Executing: {}\", threadName, work.taskName());\n        \n        eventLogger.logTaskStarted(work.executionId(), workerId, threadName);\n        \n        Instant start = Instant.now();\n        \n        try {\n            TaskResult result = taskExecutor.execute(work);\n            Duration duration = Duration.between(start, Instant.now());\n            \n            if (result.isSuccess()) {\n                logger.info(\"[{}] Completed: {} ({})\", \n                    threadName, work.taskName(), duration);\n                \n                eventLogger.logTaskCompleted(\n                    work.executionId(), result.data(), duration);\n            } else {\n                logger.error(\"[{}] Failed: {}\", threadName, work.taskName());\n                \n                eventLogger.logTaskFailed(\n                    work.executionId(), result.error(), work.attempt());\n            }\n            \n        } catch (Exception e) {\n            logger.error(\"[{}] Exception: {}\", threadName, work.taskName(), e);\n            \n            eventLogger.logTaskFailed(\n                work.executionId(), e.getMessage(), work.attempt());\n        }\n    }\n}\n\n\n\n@ApplicationScoped\npublic class TaskExecutor {\n    @ConfigProperty(name = \"orchestrator.dev.trial-run\")\n    boolean trialRun;\n    \n    @Inject\n    ExpressionEvaluator expressionEvaluator;\n    \n    public TaskResult execute(WorkMessage work) {\n        // Evaluate all expressions\n        String command = expressionEvaluator.evaluate(\n            work.command(), work.context());\n        \n        List&lt;String&gt; args = work.args().stream()\n            .map(arg -&gt; expressionEvaluator.evaluate(arg, work.context()))\n            .toList();\n        \n        Map&lt;String, String&gt; env = work.env().entrySet().stream()\n            .collect(Collectors.toMap(\n                Map.Entry::getKey,\n                e -&gt; expressionEvaluator.evaluate(e.getValue(), work.context())\n            ));\n        \n        // Build full command\n        List&lt;String&gt; fullCommand = new ArrayList&lt;&gt;();\n        fullCommand.add(command);\n        fullCommand.addAll(args);\n        \n        // Trial run mode\n        if (trialRun) {\n            return executeTrialRun(fullCommand, env, work);\n        }\n        \n        // Real execution\n        return executeCommand(fullCommand, env, work);\n    }\n    \n    private TaskResult executeTrialRun(\n        List&lt;String&gt; command, \n        Map&lt;String, String&gt; env,\n        WorkMessage work\n    ) {\n        String commandStr = String.join(\" \", command);\n        \n        logger.info(\"═══════════════════════════════════════\");\n        logger.info(\"TRIAL RUN - Would execute:\");\n        logger.info(\"  Command: {}\", commandStr);\n        logger.info(\"  Timeout: {}s\", work.timeoutSeconds());\n        logger.info(\"  Environment:\");\n        env.forEach((k, v) -&gt; logger.info(\"    {}={}\", k, v));\n        logger.info(\"═══════════════════════════════════════\");\n        \n        // Return fake success\n        return TaskResult.success(Map.of(\n            \"trial_run\", true,\n            \"command\", commandStr,\n            \"would_timeout\", work.timeoutSeconds()\n        ));\n    }\n    \n    private TaskResult executeCommand(\n        List&lt;String&gt; command,\n        Map&lt;String, String&gt; env,\n        WorkMessage work\n    ) {\n        try {\n            ProcessBuilder pb = new ProcessBuilder(command);\n            \n            // Set environment\n            pb.environment().putAll(env);\n            \n            // Redirect stderr to stdout\n            pb.redirectErrorStream(true);\n            \n            // Start process\n            Process process = pb.start();\n            \n            // Capture output\n            StringBuilder output = new StringBuilder();\n            try (BufferedReader reader = new BufferedReader(\n                    new InputStreamReader(process.getInputStream()))) {\n                String line;\n                while ((line = reader.readLine()) != null) {\n                    output.append(line).append(\"\\n\");\n                    logger.info(\"[TASK OUTPUT] {}\", line);\n                }\n            }\n            \n            // Wait for completion with timeout\n            boolean completed = process.waitFor(\n                work.timeoutSeconds(), \n                TimeUnit.SECONDS\n            );\n            \n            if (!completed) {\n                process.destroyForcibly();\n                return TaskResult.failure(\n                    \"Task timed out after \" + work.timeoutSeconds() + \"s\");\n            }\n            \n            int exitCode = process.exitValue();\n            \n            if (exitCode == 0) {\n                // Try to parse last line as JSON for downstream tasks\n                Map&lt;String, Object&gt; result = tryParseJsonOutput(\n                    output.toString());\n                \n                return TaskResult.success(result);\n            } else {\n                return TaskResult.failure(\n                    \"Task exited with code \" + exitCode + \"\\n\" + \n                    output.toString());\n            }\n            \n        } catch (IOException e) {\n            return TaskResult.failure(\n                \"Failed to start process: \" + e.getMessage());\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            return TaskResult.failure(\"Task interrupted\");\n        }\n    }\n    \n    private Map&lt;String, Object&gt; tryParseJsonOutput(String output) {\n        try {\n            String[] lines = output.split(\"\\n\");\n            if (lines.length == 0) {\n                return Map.of(\"output\", output);\n            }\n            \n            String lastLine = lines[lines.length - 1].trim();\n            \n            if (lastLine.startsWith(\"{\") && lastLine.endsWith(\"}\")) {\n                ObjectMapper mapper = new ObjectMapper();\n                return mapper.readValue(lastLine, \n                    new TypeReference&lt;Map&lt;String, Object&gt;&gt;() {});\n            }\n        } catch (Exception e) {\n            // Not JSON, that's fine\n        }\n        \n        return Map.of(\"output\", output);\n    }\n}\n\n\n\n@ApplicationScoped\npublic class EventStore {\n    @ConfigProperty(name = \"orchestrator.storage.events\")\n    String eventsPath;\n    \n    @Inject\n    StorageAdapter storageAdapter;\n    \n    private final DateTimeFormatter dateFormat = \n        DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\n    \n    public void append(Event event) {\n        String date = dateFormat.format(\n            LocalDate.ofInstant(event.timestamp(), ZoneOffset.UTC));\n        \n        String filename = date + \".jsonl\";\n        String path = eventsPath + \"/\" + filename;\n        \n        String json = toJson(event) + \"\\n\";\n        \n        storageAdapter.append(path, json);\n    }\n    \n    public Stream&lt;Event&gt; readSince(Instant since) {\n        LocalDate sinceDate = LocalDate.ofInstant(since, ZoneOffset.UTC);\n        LocalDate today = LocalDate.now();\n        \n        return sinceDate.datesUntil(today.plusDays(1))\n            .flatMap(date -&gt; {\n                String filename = dateFormat.format(date) + \".jsonl\";\n                String path = eventsPath + \"/\" + filename;\n                \n                try {\n                    return storageAdapter.readLines(path)\n                        .map(this::parseEvent)\n                        .filter(e -&gt; e.timestamp().isAfter(since));\n                } catch (IOException e) {\n                    logger.warn(\"Could not read events from {}\", path);\n                    return Stream.empty();\n                }\n            });\n    }\n    \n    public Optional&lt;Event&gt; findLastEvent(String eventType) {\n        // Search backwards from today\n        LocalDate date = LocalDate.now();\n        \n        for (int i = 0; i &lt; 30; i++) {  // Search last 30 days\n            String filename = dateFormat.format(date) + \".jsonl\";\n            String path = eventsPath + \"/\" + filename;\n            \n            try {\n                List&lt;Event&gt; events = storageAdapter.readLines(path)\n                    .map(this::parseEvent)\n                    .filter(e -&gt; e.eventType().equals(eventType))\n                    .toList();\n                \n                if (!events.isEmpty()) {\n                    return Optional.of(events.get(events.size() - 1));\n                }\n            } catch (IOException e) {\n                // File doesn't exist, continue\n            }\n            \n            date = date.minusDays(1);\n        }\n        \n        return Optional.empty();\n    }\n    \n    private String toJson(Event event) {\n        try {\n            ObjectMapper mapper = new ObjectMapper();\n            return mapper.writeValueAsString(event);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(\"Failed to serialize event\", e);\n        }\n    }\n    \n    private Event parseEvent(String json) {\n        try {\n            ObjectMapper mapper = new ObjectMapper();\n            return mapper.readValue(json, Event.class);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(\"Failed to parse event\", e);\n        }\n    }\n}\n\n\n\npublic interface StorageAdapter {\n    void append(String path, String content);\n    Stream&lt;String&gt; readLines(String path) throws IOException;\n}\n\n@ApplicationScoped\npublic class StorageAdapterFactory {\n    public StorageAdapter create(String path) {\n        if (path.startsWith(\"gs://\")) {\n            return new GcsStorageAdapter();\n        } else if (path.startsWith(\"s3://\")) {\n            return new S3StorageAdapter();\n        } else if (path.startsWith(\"file://\") || path.startsWith(\"./\")) {\n            return new LocalFilesystemAdapter();\n        } else {\n            throw new IllegalArgumentException(\n                \"Unsupported storage path: \" + path);\n        }\n    }\n}\n\n@ApplicationScoped\npublic class LocalFilesystemAdapter implements StorageAdapter {\n    @Override\n    public void append(String path, String content) {\n        Path filePath = Path.of(path.replace(\"file://\", \"\"));\n        \n        // Create directory if needed\n        Files.createDirectories(filePath.getParent());\n        \n        // Append to file\n        Files.writeString(filePath, content, \n            StandardOpenOption.CREATE, \n            StandardOpenOption.APPEND);\n    }\n    \n    @Override\n    public Stream&lt;String&gt; readLines(String path) throws IOException {\n        Path filePath = Path.of(path.replace(\"file://\", \"\"));\n        return Files.lines(filePath);\n    }\n}\n\n@ApplicationScoped\npublic class GcsStorageAdapter implements StorageAdapter {\n    @Inject\n    Storage storage;\n    \n    @Override\n    public void append(String path, String content) {\n        // Parse gs://bucket/path\n        String pathWithoutScheme = path.substring(\"gs://\".length());\n        String[] parts = pathWithoutScheme.split(\"/\", 2);\n        String bucket = parts[0];\n        String objectPath = parts[1];\n        \n        BlobId blobId = BlobId.of(bucket, objectPath);\n        BlobInfo blobInfo = BlobInfo.newBuilder(blobId).build();\n        \n        // Check if exists\n        Blob blob = storage.get(blobId);\n        \n        if (blob != null) {\n            // Append to existing\n            byte[] existing = blob.getContent();\n            byte[] combined = concat(existing, content.getBytes());\n            storage.create(blobInfo, combined);\n        } else {\n            // Create new\n            storage.create(blobInfo, content.getBytes());\n        }\n    }\n    \n    @Override\n    public Stream&lt;String&gt; readLines(String path) throws IOException {\n        String pathWithoutScheme = path.substring(\"gs://\".length());\n        String[] parts = pathWithoutScheme.split(\"/\", 2);\n        String bucket = parts[0];\n        String objectPath = parts[1];\n        \n        BlobId blobId = BlobId.of(bucket, objectPath);\n        Blob blob = storage.get(blobId);\n        \n        if (blob == null) {\n            throw new FileNotFoundException(path);\n        }\n        \n        String content = new String(blob.getContent());\n        return content.lines();\n    }\n}",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#testing-strategy",
    "href": "index.html#testing-strategy",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Repository Tests:\n@QuarkusTest\nclass GraphRepositoryTest {\n    @Inject\n    GraphRepository graphRepo;\n    \n    @Test\n    void shouldLoadGraphFromYaml() {\n        String yaml = \"\"\"\n            name: test-graph\n            tasks:\n              - name: task1\n                command: echo\n                args:\n                  - hello\n            \"\"\";\n        \n        Graph graph = graphRepo.parseYaml(yaml);\n        \n        assertThat(graph.name()).isEqualTo(\"test-graph\");\n        assertThat(graph.tasks()).hasSize(1);\n    }\n}\nExpression Evaluator Tests:\n@QuarkusTest\nclass ExpressionEvaluatorTest {\n    @Inject\n    ExpressionEvaluator evaluator;\n    \n    @Test\n    void shouldEvaluateSimpleExpression() {\n        Context ctx = new Context(\n            Map.of(\"region\", \"us\"),\n            Map.of(),\n            Map.of()\n        );\n        \n        String result = evaluator.evaluate(\"${params.region}\", ctx);\n        \n        assertThat(result).isEqualTo(\"us\");\n    }\n    \n    @Test\n    void shouldEvaluateConditional() {\n        Context ctx = new Context(\n            Map.of(\"env\", \"prod\"),\n            Map.of(),\n            Map.of()\n        );\n        \n        String result = evaluator.evaluate(\n            \"${params.env == 'prod' ? 'production' : 'development'}\", \n            ctx\n        );\n        \n        assertThat(result).isEqualTo(\"production\");\n    }\n}\nJGraphT Service Tests:\n@QuarkusTest\nclass JGraphTServiceTest {\n    @Inject\n    JGraphTService jGraphTService;\n    \n    @Test\n    void shouldDetectCycle() {\n        Graph graph = createGraphWithCycle();\n        \n        assertThatThrownBy(() -&gt; jGraphTService.buildDAG(graph))\n            .isInstanceOf(CycleFoundException.class);\n    }\n    \n    @Test\n    void shouldFindReadyTasks() {\n        // A -&gt; B -&gt; C\n        Graph graph = createLinearGraph();\n        DirectedAcyclicGraph&lt;TaskNode, DefaultEdge&gt; dag = \n            jGraphTService.buildDAG(graph);\n        \n        Map&lt;TaskNode, TaskStatus&gt; state = Map.of(\n            taskA, TaskStatus.COMPLETED,\n            taskB, TaskStatus.PENDING,\n            taskC, TaskStatus.PENDING\n        );\n        \n        Set&lt;TaskNode&gt; ready = jGraphTService.findReadyTasks(dag, state);\n        \n        assertThat(ready).containsOnly(taskB);\n    }\n}\n\n\n\nFull Graph Execution:\n@QuarkusTest\n@TestProfile(IntegrationTestProfile.class)\nclass GraphExecutionIntegrationTest {\n    @Inject\n    GraphEvaluator evaluator;\n    @Inject\n    GraphRepository graphRepo;\n    @Inject\n    WorkerPool workerPool;\n    \n    @BeforeEach\n    void setup() {\n        // Start embedded worker\n        workerPool.start(2);\n    }\n    \n    @Test\n    void shouldExecuteSimpleGraph() {\n        // Load test graph\n        Graph graph = graphRepo.loadFromFile(\"test-graphs/simple.yaml\");\n        \n        // Create execution\n        GraphExecution exec = graphExecRepo.create(\n            graph.id(),\n            Map.of(\"date\", \"2025-10-17\"),\n            \"test\"\n        );\n        \n        // Trigger evaluation\n        evaluator.evaluate(exec.id());\n        \n        // Wait for completion\n        await().atMost(30, SECONDS).until(() -&gt; {\n            GraphExecution updated = graphExecRepo.findById(exec.id());\n            return updated.status() == GraphStatus.COMPLETED;\n        });\n        \n        // Verify all tasks completed\n        List&lt;TaskExecution&gt; tasks = \n            taskExecRepo.findByGraphExecution(exec.id());\n        \n        assertThat(tasks).allMatch(te -&gt; \n            te.status() == TaskStatus.COMPLETED);\n    }\n    \n    @Test\n    void shouldHandleGlobalTaskDeduplication() {\n        // Create two graphs that use same global task\n        Graph graph1 = graphRepo.loadFromFile(\"test-graphs/with-global-1.yaml\");\n        Graph graph2 = graphRepo.loadFromFile(\"test-graphs/with-global-2.yaml\");\n        \n        Map&lt;String, Object&gt; params = Map.of(\"date\", \"2025-10-17\");\n        \n        // Start both executions\n        GraphExecution exec1 = graphExecRepo.create(\n            graph1.id(), params, \"test\");\n        GraphExecution exec2 = graphExecRepo.create(\n            graph2.id(), params, \"test\");\n        \n        evaluator.evaluate(exec1.id());\n        evaluator.evaluate(exec2.id());\n        \n        // Wait for completion\n        await().atMost(30, SECONDS).until(() -&gt; {\n            GraphExecution e1 = graphExecRepo.findById(exec1.id());\n            GraphExecution e2 = graphExecRepo.findById(exec2.id());\n            return e1.status() == GraphStatus.COMPLETED &&\n                   e2.status() == GraphStatus.COMPLETED;\n        });\n        \n        // Verify only ONE global task execution occurred\n        List&lt;GlobalTaskExecution&gt; globalExecs = \n            globalTaskRepo.findByResolvedKey(\"load_data_2025-10-17\");\n        \n        assertThat(globalExecs).hasSize(1);\n        assertThat(globalExecs.get(0).linkedGraphExecutions())\n            .containsExactlyInAnyOrder(exec1.id(), exec2.id());\n    }\n}\nTrial Run Test:\n@QuarkusTest\n@TestProfile(TrialRunTestProfile.class)\nclass TrialRunTest {\n    @Inject\n    TaskExecutor taskExecutor;\n    \n    @Test\n    void shouldLogCommandWithoutExecuting() {\n        WorkMessage work = new WorkMessage(\n            UUID.randomUUID(),\n            \"test-task\",\n            \"python\",\n            List.of(\"script.py\", \"--dangerous\"),\n            Map.of(\"API_KEY\", \"secret\"),\n            600\n        );\n        \n        TaskResult result = taskExecutor.execute(work);\n        \n        // Should succeed without actually running\n        assertThat(result.isSuccess()).isTrue();\n        assertThat(result.data()).containsEntry(\"trial_run\", true);\n        assertThat(result.data()).containsEntry(\"command\", \n            \"python script.py --dangerous\");\n        \n        // Verify nothing was actually executed (check logs)\n    }\n}\n\n\n\nUse Testcontainers for full stack:\n@QuarkusTest\n@TestProfile(E2ETestProfile.class)\nclass EndToEndTest {\n    @Container\n    static HazelcastContainer hazelcast = \n        new HazelcastContainer(\"hazelcast/hazelcast:5.3\");\n    \n    @Test\n    void shouldExecuteGraphEndToEnd() {\n        // Deploy graph via API\n        given()\n            .contentType(\"application/yaml\")\n            .body(loadTestGraph())\n        .when()\n            .post(\"/api/graphs\")\n        .then()\n            .statusCode(201);\n        \n        // Trigger execution\n        String executionId = given()\n            .contentType(\"application/json\")\n            .body(Map.of(\"params\", Map.of(\"date\", \"2025-10-17\")))\n        .when()\n            .post(\"/api/graphs/test-graph/execute\")\n        .then()\n            .statusCode(200)\n            .extract()\n            .path(\"executionId\");\n        \n        // Poll for completion\n        await().atMost(60, SECONDS).until(() -&gt; {\n            String status = given()\n                .when()\n                    .get(\"/api/graphs/executions/\" + executionId)\n                .then()\n                    .statusCode(200)\n                    .extract()\n                    .path(\"status\");\n            \n            return \"COMPLETED\".equals(status);\n        });\n        \n        // Verify events logged\n        String events = given()\n            .when()\n                .get(\"/api/events?execution=\" + executionId)\n            .then()\n                .statusCode(200)\n                .extract()\n                .asString();\n        \n        assertThat(events).contains(\"GRAPH_STARTED\");\n        assertThat(events).contains(\"TASK_COMPLETED\");\n        assertThat(events).contains(\"GRAPH_COMPLETED\");\n    }\n}",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#deployment-guide",
    "href": "index.html#deployment-guide",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "Prerequisites: - Java 21+ - Maven or Gradle\nSteps:\n\nClone repository\n\ngit clone https://github.com/yourorg/orchestrator.git\ncd orchestrator\n\nBuild\n\n./mvnw clean package\n\nCreate config directories\n\nmkdir -p graphs tasks data/events\n\nCreate sample graph\n\ncat &gt; graphs/hello-world.yaml &lt;&lt;EOF\nname: hello-world\ntasks:\n  - name: say-hello\n    command: echo\n    args:\n      - \"Hello, World!\"\nEOF\n\nRun\n\njava -jar target/quarkus-app/quarkus-run.jar\n\nOpen browser\n\nopen http://localhost:8080\n\n\n\nBuild:\ndocker build -t orchestrator:latest .\nRun:\ndocker run -p 8080:8080 \\\n  -v $(pwd)/graphs:/config/graphs \\\n  -v $(pwd)/tasks:/config/tasks \\\n  -v $(pwd)/data:/data \\\n  orchestrator:latest\n\n\n\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  hazelcast:\n    image: hazelcast/hazelcast:5.3\n    environment:\n      JAVA_OPTS: \"-Xms512m -Xmx512m\"\n    ports:\n      - \"5701:5701\"\n  \n  orchestrator:\n    build: .\n    environment:\n      ORCHESTRATOR_MODE: prod\n      HAZELCAST_MEMBERS: hazelcast:5701\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./graphs:/config/graphs:ro\n      - ./tasks:/config/tasks:ro\n      - ./data:/data\n    depends_on:\n      - hazelcast\n  \n  worker:\n    build: .\n    command: [\"worker\"]\n    environment:\n      WORKER_THREADS: 8\n      HAZELCAST_MEMBERS: hazelcast:5701\n    volumes:\n      - ./graphs:/config/graphs:ro\n      - ./tasks:/config/tasks:ro\n    depends_on:\n      - hazelcast\n    deploy:\n      replicas: 2\n\n\n\nPrerequisites: - GKE cluster - kubectl configured - Container images pushed to GCR\nDeploy Hazelcast:\nkubectl apply -f k8s/hazelcast-statefulset.yaml\nCreate ConfigMap:\nkubectl create configmap workflow-config \\\n  --from-file=graphs/ \\\n  --from-file=tasks/ \\\n  -n orchestrator\nDeploy Orchestrator:\nkubectl apply -f k8s/orchestrator-deployment.yaml\nDeploy Workers:\nkubectl apply -f k8s/worker-deployment.yaml\nVerify:\nkubectl get pods -n orchestrator\nkubectl logs -n orchestrator deployment/orchestrator\nAccess UI:\nkubectl port-forward -n orchestrator svc/orchestrator 8080:80\nopen http://localhost:8080",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "A lightweight, portable, event-driven workflow orchestrator that:\n\nRuns anywhere - Single binary, no dependencies\nUses simple YAML - No Python code required\nDeduplicates work - Global tasks run once, notify many\nScales efficiently - Multi-threaded workers for I/O workloads\nHas great DX - Dev mode, trial run, hot reload, web editor\nIs observable - Events in files, metrics, real-time UI\nGets out of your way - Simple, fast, focused\n\n\n\n\n✅ Global Tasks - Run once per parameter set, notify all graphs ✅ Parameter Scoping - Clean override hierarchy ✅ JEXL Expressions - Powerful, safe templating ✅ Single Process Dev Mode - Everything in one process ✅ Trial Run - Test without executing ✅ Web Editor - Edit YAML in browser (dev only) ✅ File-Based Events - JSONL files, not databases ✅ Multi-Threaded Workers - Efficient for GCP workloads ✅ Tabler UI - Professional, polished interface ✅ Hazelcast State - Fast, distributed state management\n\n\n\n\nLanguage: Java 21\nFramework: Quarkus\nState: Hazelcast (embedded in dev, clustered in prod)\nEvents: JSONL files (local/GCS/S3)\nUI: Qute + Tabler CSS + Monaco Editor\nExpressions: Apache Commons JEXL\nDAG: JGraphT\nDeployment: Single binary or GKE\n\n\n\n\n\nCreate GitHub repository\nGenerate feature issues from this design\nSetup CI/CD pipeline (GitHub Actions)\nImplement core features in phases\nTest extensively (unit, integration, E2E)\nDeploy to GKE\nIterate based on feedback",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#appendix-example-yaml-files",
    "href": "index.html#appendix-example-yaml-files",
    "title": "Event-Driven Workflow Orchestrator - Design Document",
    "section": "",
    "text": "# tasks/load-market-data.yaml\nname: load-market-data\nglobal: true\nkey: \"load_market_${params.batch_date}_${params.region}\"\n\nparams:\n  batch_date:\n    type: string\n    required: true\n    description: Business date to process (YYYY-MM-DD)\n  \n  region:\n    type: string\n    default: us\n    description: Market region (us, eu, asia)\n\ncommand: dbt\nargs:\n  - run\n  - --models\n  - +market_data\n  - --vars\n  - \"batch_date:${params.batch_date},region:${params.region}\"\n  - --target\n  - prod\n\nenv:\n  DBT_PROFILES_DIR: /dbt/profiles\n  DBT_TARGET: prod\n\ntimeout: 600\nretry: 3\n\n\n\n# graphs/daily-risk-calculation.yaml\nname: daily-risk-calculation\ndescription: |\n  Daily risk calculation pipeline.\n  Runs after market close to calculate VaR and stress scenarios.\n\nparams:\n  batch_date:\n    type: string\n    default: \"${date.today()}\"\n    description: Processing date\n  \n  region:\n    type: string\n    default: us\n    description: Market region\n  \n  confidence_level:\n    type: number\n    default: 0.99\n    description: VaR confidence level\n\nenv:\n  GCS_BUCKET: \"gs://risk-data-${params.region}\"\n  PROCESSING_DATE: \"${params.batch_date}\"\n\nschedule: \"0 18 * * 1-5\"  # 6 PM, weekdays only\n\ntasks:\n  # Global task - shared with other graphs\n  - task: load-market-data\n    params:\n      batch_date: \"${params.batch_date}\"\n      region: \"${params.region}\"\n  \n  # Inline task - specific to this graph\n  - name: calculate-var\n    command: python\n    args:\n      - /opt/risk/var.py\n      - --date\n      - \"${params.batch_date}\"\n      - --region\n      - \"${params.region}\"\n      - --confidence\n      - \"${params.confidence_level}\"\n      - --output\n      - \"${env.GCS_BUCKET}/var/${params.batch_date}.parquet\"\n    env:\n      PYTHONUNBUFFERED: \"1\"\n    timeout: 1800\n    retry: 2\n    depends_on:\n      - load-market-data\n  \n  - name: stress-test\n    command: python\n    args:\n      - /opt/risk/stress.py\n      - --date\n      - \"${params.batch_date}\"\n      - --scenarios\n      - \"recession,spike,crash\"\n      - --input\n      - \"${env.GCS_BUCKET}/var/${params.batch_date}.parquet\"\n    timeout: 3600\n    depends_on:\n      - calculate-var\n  \n  - name: generate-report\n    command: python\n    args:\n      - /opt/risk/report.py\n      - --date\n      - \"${params.batch_date}\"\n      - --var-results\n      - \"${task.calculate-var.result.output_file}\"\n      - --stress-results\n      - \"${task.stress-test.result.output_file}\"\n    depends_on:\n      - calculate-var\n      - stress-test\n\n\n\n# application.yaml\norchestrator:\n  mode: dev\n  \n  config:\n    graphs: ./graphs\n    tasks: ./tasks\n    watch: true\n  \n  dev:\n    worker-threads: 4\n    trial-run: false\n    enable-editor: true\n  \n  storage:\n    events: file://./data/events\n  \n  hazelcast:\n    embedded: true\n    cluster-name: orchestrator-dev\n  \n  worker:\n    threads: 4\n    heartbeat-interval: 10\n    dead-threshold: 60\n\nserver:\n  port: 8080\n\nlogging:\n  level: INFO\n  format: text\n\n---\n\n# application-prod.yaml\norchestrator:\n  mode: prod\n  \n  config:\n    graphs: /config/graphs\n    tasks: /config/tasks\n    watch: false\n  \n  storage:\n    events: gs://my-bucket/orchestrator/events\n  \n  hazelcast:\n    embedded: false\n    cluster-name: orchestrator-prod\n    members:\n      - hazelcast-0.hazelcast.orchestrator.svc.cluster.local\n      - hazelcast-1.hazelcast.orchestrator.svc.cluster.local\n      - hazelcast-2.hazelcast.orchestrator.svc.cluster.local\n  \n  worker:\n    threads: 16\n    heartbeat-interval: 10\n    dead-threshold: 60\n\nserver:\n  port: 8080\n\nlogging:\n  level: INFO\n  format: json\n\nEND OF DESIGN DOCUMENT",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "expressions.html",
    "href": "expressions.html",
    "title": "Expression Language Guide",
    "section": "",
    "text": "The orchestrator uses Apache Commons JEXL 3 for dynamic expression evaluation. Expressions allow you to parameterize your workflows and make them adaptable to different contexts.\n\n\nExpressions are wrapped in ${ }:\ncommand: echo\nargs:\n  - \"Hello ${params.name}\"\nIf a string doesn’t contain ${ }, it’s treated as a literal value.\n\n\n\n\n\nAccess workflow parameters with params:\nargs:\n  - --date\n  - \"${params.batch_date}\"\n  - --region\n  - \"${params.region}\"\n\n\n\nAccess environment variables with env:\nenv:\n  API_KEY: \"${env.API_KEY}\"\n  DATABASE_URL: \"${env.DATABASE_URL}\"\n\n\n\nAccess results from upstream tasks with task:\nargs:\n  - --row-count\n  - \"${task.extract.result.row_count}\"\n  - --output\n  - \"${task.extract.result.output_file}\"\n\n\n\n\n\n\nenv:\n  BATCH_SIZE: \"${params.scale * 100}\"\n  WORKER_COUNT: \"${params.scale * 4}\"\n  TOTAL: \"${params.a + params.b}\"\n  PERCENT: \"${params.value / params.total * 100}\"\n\n\n\n# Equal\ncondition: \"${params.env == 'prod'}\"\n\n# Not equal\ncondition: \"${params.status != 'failed'}\"\n\n# Greater than / less than\ncondition: \"${params.count &gt; 100}\"\ncondition: \"${params.age &gt;= 18}\"\n\n\n\n# AND\ncondition: \"${params.enabled && params.ready}\"\n\n# OR\ncondition: \"${params.force || params.override}\"\n\n# NOT\ncondition: \"${!params.disabled}\"\n\n\n\nargs:\n  - \"${params.full_refresh ? '--full-refresh' : '--incremental'}\"\n  \nenv:\n  MODE: \"${params.env == 'prod' ? 'production' : 'development'}\"\n  PARALLELISM: \"${params.scale &gt; 10 ? 32 : params.scale &gt; 5 ? 16 : 4}\"\n\n\n\n\n\n\nenv:\n  TABLE_NAME: \"${'data_' + params.region + '_' + params.date}\"\n  OUTPUT_PATH: \"${params.bucket + '/' + params.path}\"\n\n\n\n# Convert to uppercase\nenv:\n  ENV_UPPER: \"${params.env.toUpperCase()}\"\n\n# Convert to lowercase\nenv:\n  ENV_LOWER: \"${params.env.toLowerCase()}\"\n\n# Replace characters\nenv:\n  DATE_UNDERSCORED: \"${params.date.replace('-', '_')}\"\n\n# Trim whitespace\nenv:\n  CLEAN_VALUE: \"${params.value.trim()}\"\n\n\n\nEmbed multiple expressions in a single string:\nargs:\n  - \"gs://bucket-${params.region}/data/${params.date}/file.csv\"\n  - \"Processing ${params.count} records for ${params.client}\"\n\n\n\n\n\n\nUse ?. to safely access nested properties:\n# Won't throw if config, api, or key is null\nenv:\n  API_KEY: \"${params.config?.api?.key}\"\n\n\n\nProvide default values for null or missing variables:\nenv:\n  REGION: \"${params.region ?: 'us'}\"\n  TIMEOUT: \"${params.timeout ?: 300}\"\n  API_URL: \"${env.API_URL ?: 'https://api.default.com'}\"\n\n\n\n\n\n\n\n\nGet today’s date in ISO format (yyyy-MM-dd):\nparams:\n  batch_date:\n    default: \"${date.today()}\"\n\n\n\nGet current date/time with custom formatting:\nenv:\n  TIMESTAMP: \"${date.now('yyyy-MM-dd HH:mm:ss')}\"\n  DATE_PATH: \"${date.now('yyyy/MM/dd')}\"\n\n\n\nAdd time to a date:\nparams:\n  tomorrow:\n    default: \"${date.add(date.today(), 1, 'days')}\"\n  next_week:\n    default: \"${date.add(date.today(), 7, 'days')}\"\n  next_month:\n    default: \"${date.add(date.today(), 1, 'months')}\"\nUnits: days, weeks, months, years\n\n\n\nSubtract time from a date:\nparams:\n  yesterday:\n    default: \"${date.sub(date.today(), 1, 'days')}\"\n  last_week:\n    default: \"${date.sub(date.today(), 7, 'days')}\"\n\n\n\nFormat a date string:\nenv:\n  DATE_SLASH: \"${date.format(params.date, 'yyyy/MM/dd')}\"\n  DATE_READABLE: \"${date.format(params.date, 'dd-MMM-yyyy')}\"\n\n\n\nCalculate days between two dates:\nenv:\n  DAYS_DIFF: \"${date.daysBetween('2025-10-01', '2025-10-17')}\"\n\n\n\nCompare dates:\ncondition: \"${date.isBefore(params.date, '2025-12-31')}\"\n\n\n\n\n\n\nGenerate a random UUID:\nenv:\n  RUN_ID: \"${string.uuid()}\"\n  TRACE_ID: \"${string.uuid()}\"\n\n\n\nConvert text to URL-safe slug:\nenv:\n  WORKFLOW_SLUG: \"${string.slugify('My Workflow Name')}\"\n  # Result: \"my-workflow-name\"\n\n\n\nJoin multiple strings:\nenv:\n  PATH: \"${string.join('/', params.bucket, params.path, params.file)}\"\n\n\n\nPad strings to a specific length:\nenv:\n  ORDER_ID: \"${string.padLeft(params.id, 8, '0')}\"\n  # \"42\" becomes \"00000042\"\n\n\n\nConvert to snake_case:\nenv:\n  VAR_NAME: \"${string.toSnakeCase('myVariableName')}\"\n  # Result: \"my_variable_name\"\n\n\n\nConvert to camelCase:\nenv:\n  VAR_NAME: \"${string.toCamelCase('my_variable_name')}\"\n  # Result: \"myVariableName\"\n\n\n\n\nAccess standard Java Math functions:\nenv:\n  ROUNDED: \"${Math.round(3.7)}\"           # 4\n  FLOOR: \"${Math.floor(3.7)}\"             # 3.0\n  CEIL: \"${Math.ceil(3.2)}\"               # 4.0\n  MAX: \"${Math.max(params.a, params.b)}\"  # larger value\n  MIN: \"${Math.min(params.a, params.b)}\"  # smaller value\n  ABS: \"${Math.abs(params.value)}\"        # absolute value\n\n\n\n\n\n\nargs:\n  - --input\n  - \"gs://${params.bucket}/raw/${params.date}/*.parquet\"\n  - --output\n  - \"gs://${params.bucket}/processed/${params.date}/\"\n\n\n\nargs:\n  - dbt\n  - run\n  - \"${params.full_refresh ? '--full-refresh' : ''}\"\n  - \"${params.threads ? '--threads' : ''}\"\n  - \"${params.threads ?: ''}\"\n\n\n\nenv:\n  DATABASE_URL: \"${params.env == 'prod' ? env.PROD_DB_URL : env.DEV_DB_URL}\"\n  LOG_LEVEL: \"${params.env == 'prod' ? 'INFO' : 'DEBUG'}\"\n  WORKERS: \"${params.env == 'prod' ? 16 : 4}\"\n\n\n\nargs:\n  - --partition\n  - \"${date.format(params.date, 'yyyy/MM/dd')}\"\n  - --date-range\n  - \"${date.sub(params.date, 7, 'days')}\"\n  - to\n  - \"${params.date}\"\n\n\n\nkey: \"load_${params.dataset}_${params.date}_${params.region}\"\n\n\n\nparams:\n  batch_date:\n    default: \"${params.override_date ?: date.today()}\"\n  \n  timeout:\n    default: \"${params.custom_timeout ?: env.DEFAULT_TIMEOUT ?: 3600}\"\n\n\n\nenv:\n  # Calculate memory allocation based on scale\n  MEMORY_GB: \"${Math.min(Math.max(params.scale * 2, 4), 64)}\"\n  \n  # Dynamic parallelism\n  PARALLELISM: \"${params.dataset == 'large' ? 32 : params.dataset == 'medium' ? 16 : 8}\"\n\n\n\n\n\n\nGood:\nenv:\n  DATE: \"${params.date}\"\n  REGION: \"${params.region}\"\nAvoid:\nenv:\n  COMPLEX: \"${params.a + params.b * params.c &gt; 100 ? 'high' : params.d ? 'medium' : 'low'}\"\nFor complex logic, break it into multiple parameters or use task code.\n\n\n\nGood:\nparams:\n  batch_date:\n    default: \"${date.today()}\"\n  processing_region:\n    default: \"us\"\nAvoid:\nparams:\n  d:\n    default: \"${date.today()}\"\n  r:\n    default: \"us\"\n\n\n\nAlways mark required parameters explicitly:\nparams:\n  batch_date:\n    type: string\n    required: true\n    description: \"Processing date in YYYY-MM-DD format\"\n\n\n\nAdd descriptions to parameters with complex defaults:\nparams:\n  lookback_date:\n    type: string\n    default: \"${date.sub(date.today(), 7, 'days')}\"\n    description: \"Date 7 days ago for lookback window\"\n\n\n\nProvide sensible defaults:\nparams:\n  timeout:\n    default: \"${params.custom_timeout ?: env.DEFAULT_TIMEOUT ?: 3600}\"\n    description: \"Task timeout in seconds (default: 1 hour)\"\n\n\n\nUse trial run mode to verify expressions evaluate correctly:\n# Enable trial run in application-dev.yaml\norchestrator:\n  dev:\n    trial-run: true\n\n\n\n\n\n\nProblem: Expression evaluates to null\nSolutions: - Check variable names are spelled correctly - Verify the variable exists in context (params, env, task) - Use elvis operator for default: ${params.value ?: 'default'} - Use null-safe navigation: ${params.config?.api?.key}\n\n\n\nProblem: Failed to evaluate expression error\nSolutions: - Ensure expression is wrapped in ${ } - Check for balanced braces and quotes - Verify function names are correct - Test with simpler expression first\n\n\n\nProblem: Operation fails due to type mismatch\nSolutions: - Use explicit type conversion if needed - Check parameter types in definition - Remember that all params are initially strings from YAML\n\n\n\nProblem: URLs with :// cause parsing errors\nSolution: This is handled automatically - the evaluator correctly handles strings like:\n\"gs://bucket-${params.region}/data/${params.date}/file.csv\"\n\n\n\n\n\nExpression Caching: Up to 512 compiled expressions are cached automatically\nRepeated Evaluations: Same expressions in loops benefit from caching\nComplex Expressions: Break very complex expressions into multiple simpler ones\nString Concatenation: Multiple small concatenations are optimized by JEXL\n\n\n\n\n\nExpressions run with unrestricted permissions for flexibility\nOnly use trusted YAML configurations\nAvoid exposing sensitive data in expression results that might be logged\nUse environment variables for secrets, not parameters",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#basic-syntax",
    "href": "expressions.html#basic-syntax",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Expressions are wrapped in ${ }:\ncommand: echo\nargs:\n  - \"Hello ${params.name}\"\nIf a string doesn’t contain ${ }, it’s treated as a literal value.",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#variables",
    "href": "expressions.html#variables",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Access workflow parameters with params:\nargs:\n  - --date\n  - \"${params.batch_date}\"\n  - --region\n  - \"${params.region}\"\n\n\n\nAccess environment variables with env:\nenv:\n  API_KEY: \"${env.API_KEY}\"\n  DATABASE_URL: \"${env.DATABASE_URL}\"\n\n\n\nAccess results from upstream tasks with task:\nargs:\n  - --row-count\n  - \"${task.extract.result.row_count}\"\n  - --output\n  - \"${task.extract.result.output_file}\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#operators",
    "href": "expressions.html#operators",
    "title": "Expression Language Guide",
    "section": "",
    "text": "env:\n  BATCH_SIZE: \"${params.scale * 100}\"\n  WORKER_COUNT: \"${params.scale * 4}\"\n  TOTAL: \"${params.a + params.b}\"\n  PERCENT: \"${params.value / params.total * 100}\"\n\n\n\n# Equal\ncondition: \"${params.env == 'prod'}\"\n\n# Not equal\ncondition: \"${params.status != 'failed'}\"\n\n# Greater than / less than\ncondition: \"${params.count &gt; 100}\"\ncondition: \"${params.age &gt;= 18}\"\n\n\n\n# AND\ncondition: \"${params.enabled && params.ready}\"\n\n# OR\ncondition: \"${params.force || params.override}\"\n\n# NOT\ncondition: \"${!params.disabled}\"\n\n\n\nargs:\n  - \"${params.full_refresh ? '--full-refresh' : '--incremental'}\"\n  \nenv:\n  MODE: \"${params.env == 'prod' ? 'production' : 'development'}\"\n  PARALLELISM: \"${params.scale &gt; 10 ? 32 : params.scale &gt; 5 ? 16 : 4}\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#string-operations",
    "href": "expressions.html#string-operations",
    "title": "Expression Language Guide",
    "section": "",
    "text": "env:\n  TABLE_NAME: \"${'data_' + params.region + '_' + params.date}\"\n  OUTPUT_PATH: \"${params.bucket + '/' + params.path}\"\n\n\n\n# Convert to uppercase\nenv:\n  ENV_UPPER: \"${params.env.toUpperCase()}\"\n\n# Convert to lowercase\nenv:\n  ENV_LOWER: \"${params.env.toLowerCase()}\"\n\n# Replace characters\nenv:\n  DATE_UNDERSCORED: \"${params.date.replace('-', '_')}\"\n\n# Trim whitespace\nenv:\n  CLEAN_VALUE: \"${params.value.trim()}\"\n\n\n\nEmbed multiple expressions in a single string:\nargs:\n  - \"gs://bucket-${params.region}/data/${params.date}/file.csv\"\n  - \"Processing ${params.count} records for ${params.client}\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#null-safety",
    "href": "expressions.html#null-safety",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Use ?. to safely access nested properties:\n# Won't throw if config, api, or key is null\nenv:\n  API_KEY: \"${params.config?.api?.key}\"\n\n\n\nProvide default values for null or missing variables:\nenv:\n  REGION: \"${params.region ?: 'us'}\"\n  TIMEOUT: \"${params.timeout ?: 300}\"\n  API_URL: \"${env.API_URL ?: 'https://api.default.com'}\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#built-in-functions",
    "href": "expressions.html#built-in-functions",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Get today’s date in ISO format (yyyy-MM-dd):\nparams:\n  batch_date:\n    default: \"${date.today()}\"\n\n\n\nGet current date/time with custom formatting:\nenv:\n  TIMESTAMP: \"${date.now('yyyy-MM-dd HH:mm:ss')}\"\n  DATE_PATH: \"${date.now('yyyy/MM/dd')}\"\n\n\n\nAdd time to a date:\nparams:\n  tomorrow:\n    default: \"${date.add(date.today(), 1, 'days')}\"\n  next_week:\n    default: \"${date.add(date.today(), 7, 'days')}\"\n  next_month:\n    default: \"${date.add(date.today(), 1, 'months')}\"\nUnits: days, weeks, months, years\n\n\n\nSubtract time from a date:\nparams:\n  yesterday:\n    default: \"${date.sub(date.today(), 1, 'days')}\"\n  last_week:\n    default: \"${date.sub(date.today(), 7, 'days')}\"\n\n\n\nFormat a date string:\nenv:\n  DATE_SLASH: \"${date.format(params.date, 'yyyy/MM/dd')}\"\n  DATE_READABLE: \"${date.format(params.date, 'dd-MMM-yyyy')}\"\n\n\n\nCalculate days between two dates:\nenv:\n  DAYS_DIFF: \"${date.daysBetween('2025-10-01', '2025-10-17')}\"\n\n\n\nCompare dates:\ncondition: \"${date.isBefore(params.date, '2025-12-31')}\"\n\n\n\n\n\n\nGenerate a random UUID:\nenv:\n  RUN_ID: \"${string.uuid()}\"\n  TRACE_ID: \"${string.uuid()}\"\n\n\n\nConvert text to URL-safe slug:\nenv:\n  WORKFLOW_SLUG: \"${string.slugify('My Workflow Name')}\"\n  # Result: \"my-workflow-name\"\n\n\n\nJoin multiple strings:\nenv:\n  PATH: \"${string.join('/', params.bucket, params.path, params.file)}\"\n\n\n\nPad strings to a specific length:\nenv:\n  ORDER_ID: \"${string.padLeft(params.id, 8, '0')}\"\n  # \"42\" becomes \"00000042\"\n\n\n\nConvert to snake_case:\nenv:\n  VAR_NAME: \"${string.toSnakeCase('myVariableName')}\"\n  # Result: \"my_variable_name\"\n\n\n\nConvert to camelCase:\nenv:\n  VAR_NAME: \"${string.toCamelCase('my_variable_name')}\"\n  # Result: \"myVariableName\"\n\n\n\n\nAccess standard Java Math functions:\nenv:\n  ROUNDED: \"${Math.round(3.7)}\"           # 4\n  FLOOR: \"${Math.floor(3.7)}\"             # 3.0\n  CEIL: \"${Math.ceil(3.2)}\"               # 4.0\n  MAX: \"${Math.max(params.a, params.b)}\"  # larger value\n  MIN: \"${Math.min(params.a, params.b)}\"  # smaller value\n  ABS: \"${Math.abs(params.value)}\"        # absolute value",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#common-patterns",
    "href": "expressions.html#common-patterns",
    "title": "Expression Language Guide",
    "section": "",
    "text": "args:\n  - --input\n  - \"gs://${params.bucket}/raw/${params.date}/*.parquet\"\n  - --output\n  - \"gs://${params.bucket}/processed/${params.date}/\"\n\n\n\nargs:\n  - dbt\n  - run\n  - \"${params.full_refresh ? '--full-refresh' : ''}\"\n  - \"${params.threads ? '--threads' : ''}\"\n  - \"${params.threads ?: ''}\"\n\n\n\nenv:\n  DATABASE_URL: \"${params.env == 'prod' ? env.PROD_DB_URL : env.DEV_DB_URL}\"\n  LOG_LEVEL: \"${params.env == 'prod' ? 'INFO' : 'DEBUG'}\"\n  WORKERS: \"${params.env == 'prod' ? 16 : 4}\"\n\n\n\nargs:\n  - --partition\n  - \"${date.format(params.date, 'yyyy/MM/dd')}\"\n  - --date-range\n  - \"${date.sub(params.date, 7, 'days')}\"\n  - to\n  - \"${params.date}\"\n\n\n\nkey: \"load_${params.dataset}_${params.date}_${params.region}\"\n\n\n\nparams:\n  batch_date:\n    default: \"${params.override_date ?: date.today()}\"\n  \n  timeout:\n    default: \"${params.custom_timeout ?: env.DEFAULT_TIMEOUT ?: 3600}\"\n\n\n\nenv:\n  # Calculate memory allocation based on scale\n  MEMORY_GB: \"${Math.min(Math.max(params.scale * 2, 4), 64)}\"\n  \n  # Dynamic parallelism\n  PARALLELISM: \"${params.dataset == 'large' ? 32 : params.dataset == 'medium' ? 16 : 8}\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#best-practices",
    "href": "expressions.html#best-practices",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Good:\nenv:\n  DATE: \"${params.date}\"\n  REGION: \"${params.region}\"\nAvoid:\nenv:\n  COMPLEX: \"${params.a + params.b * params.c &gt; 100 ? 'high' : params.d ? 'medium' : 'low'}\"\nFor complex logic, break it into multiple parameters or use task code.\n\n\n\nGood:\nparams:\n  batch_date:\n    default: \"${date.today()}\"\n  processing_region:\n    default: \"us\"\nAvoid:\nparams:\n  d:\n    default: \"${date.today()}\"\n  r:\n    default: \"us\"\n\n\n\nAlways mark required parameters explicitly:\nparams:\n  batch_date:\n    type: string\n    required: true\n    description: \"Processing date in YYYY-MM-DD format\"\n\n\n\nAdd descriptions to parameters with complex defaults:\nparams:\n  lookback_date:\n    type: string\n    default: \"${date.sub(date.today(), 7, 'days')}\"\n    description: \"Date 7 days ago for lookback window\"\n\n\n\nProvide sensible defaults:\nparams:\n  timeout:\n    default: \"${params.custom_timeout ?: env.DEFAULT_TIMEOUT ?: 3600}\"\n    description: \"Task timeout in seconds (default: 1 hour)\"\n\n\n\nUse trial run mode to verify expressions evaluate correctly:\n# Enable trial run in application-dev.yaml\norchestrator:\n  dev:\n    trial-run: true",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#troubleshooting",
    "href": "expressions.html#troubleshooting",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Problem: Expression evaluates to null\nSolutions: - Check variable names are spelled correctly - Verify the variable exists in context (params, env, task) - Use elvis operator for default: ${params.value ?: 'default'} - Use null-safe navigation: ${params.config?.api?.key}\n\n\n\nProblem: Failed to evaluate expression error\nSolutions: - Ensure expression is wrapped in ${ } - Check for balanced braces and quotes - Verify function names are correct - Test with simpler expression first\n\n\n\nProblem: Operation fails due to type mismatch\nSolutions: - Use explicit type conversion if needed - Check parameter types in definition - Remember that all params are initially strings from YAML\n\n\n\nProblem: URLs with :// cause parsing errors\nSolution: This is handled automatically - the evaluator correctly handles strings like:\n\"gs://bucket-${params.region}/data/${params.date}/file.csv\"",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#performance-considerations",
    "href": "expressions.html#performance-considerations",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Expression Caching: Up to 512 compiled expressions are cached automatically\nRepeated Evaluations: Same expressions in loops benefit from caching\nComplex Expressions: Break very complex expressions into multiple simpler ones\nString Concatenation: Multiple small concatenations are optimized by JEXL",
    "crumbs": [
      "Expression Reference"
    ]
  },
  {
    "objectID": "expressions.html#security-notes",
    "href": "expressions.html#security-notes",
    "title": "Expression Language Guide",
    "section": "",
    "text": "Expressions run with unrestricted permissions for flexibility\nOnly use trusted YAML configurations\nAvoid exposing sensitive data in expression results that might be logged\nUse environment variables for secrets, not parameters",
    "crumbs": [
      "Expression Reference"
    ]
  }
]